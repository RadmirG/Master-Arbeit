{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a7a419b-c1c5-46cf-90c3-3530b4dfd9ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"s0\"></a><h2 style=\"color:rgb(127,203,223)\">3. Klassifikations- und Regressionsverfahren Teil 1</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ce32d6-54c8-4ece-9cce-0534a3f768bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"s1\"></a><h3 style=\"color:rgb(127,203,223)\">3.1 Der Naive Bayes-Klassifikator</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ade47-c108-4501-bb09-d2f922acf4c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "Wir wollen mit dem naiven Bayes-Klassifikator beginnen. Eigentlich ist hierunter eine ganze Familie von so genannten probabalistischen (Wahrscheinlichkeiten verwendende) Klassifikatoren gemeint, die Gebrauch vom <a href=\"Chapter02.ipynb#s8\">Satz von Bayes</a> machen und darüber hinaus eine zusätzliche (naive) Annahme hinsichtlich der bedingten Unabhängigkeit der Merkmale voraussetzen. Wir wollen uns im Folgenden zunächst basierend auf einem einfachen Beispiel die Grundlagen des Verfahrens anschauen, bevor wir anschließend mit Hilfe von sklearn sowie basierend auf einer größeren Datenmengen ein realistisches Beispiel betrachen werden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235484f7-954f-4a14-9b82-69436dedbf1f",
   "metadata": {},
   "source": [
    "Wir gehen davon aus, dass wir einen Datensatz gegeben haben. Dieser besteht aus so genannten Merkmalen sowie einem Klassenlabel. Unter einem Merkmal können wir uns im allgemeinen Fall einen reellen Vektor vorstellen, wollen jedoch der Einfachheit halber zunächst mit diskreten Merkmalen beginnen:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf4d98c-bee7-4257-b35d-e293bac7ebac",
   "metadata": {},
   "source": [
    "Wir wollen anhand von drei Angaben (Merkmalen) hinsichtlich des Wetters entscheiden (Klassenlabel), ob wir einen Ausflug machen oder nicht. Beispiele für Ausprägungen der Merkmale sowie der zugehörigen Entscheidung (Klassenlabel) sind in der folgenden Tabelle aufgelistet:\n",
    "<a id=\"s4\"></a>\n",
    "<table>\n",
    "  <thead>  \n",
    "      <tr>\n",
    "          <th>Index</th><th>Aussicht (Merkmal 1)</th><th>Luftfeuchtigkeit (Merkmal 2)</th><th>Windstärke (Merkmal 3)</th><th>Entscheidung (Klassenlabel)</th>\n",
    "      </tr>\n",
    "    </thead> \n",
    "    <tbody>\n",
    "      <tr>\n",
    "          <td>1</td><td>sonnig</td><td>hoch</td><td>stark</td><td>Nein</td>\n",
    "      </tr>  \n",
    "     <tr>\n",
    "          <td>2</td><td>regnerisch</td><td>hoch</td><td>stark</td><td>Nein</td>\n",
    "      </tr>                  \n",
    "     <tr>\n",
    "         <td>3</td><td>regnerisch</td><td>normal</td><td>stark</td><td>Nein</td>\n",
    "      </tr>  \n",
    "      <tr>\n",
    "          <td>4</td><td>sonnig</td><td>normal</td><td>stark</td><td>Ja</td>\n",
    "      </tr>  \n",
    "     <tr>\n",
    "          <td>5</td><td>bewölkt</td><td>hoch</td><td>stark</td><td>Ja</td>\n",
    "      </tr>          \n",
    "     <tr>\n",
    "          <td>6</td><td>bewölkt</td><td>normal</td><td>stark</td><td>Ja</td>\n",
    "      </tr>  \n",
    "     <tr>\n",
    "          <td>7</td><td>sonnig</td><td>hoch</td><td>schwach</td><td>Nein</td>\n",
    "      </tr>  \n",
    "      <tr>\n",
    "          <td>8</td><td>sonnig</td><td>normal</td><td>schwach</td><td>Ja</td>\n",
    "      </tr>                    \n",
    "     <tr>\n",
    "          <td>9</td><td>regnerisch</td><td>hoch</td><td>schwach</td><td>Ja</td>\n",
    "      </tr>  \n",
    "     <tr>\n",
    "          <td>10</td><td>regnerisch</td><td>normal</td><td>schwach</td><td>Ja</td>\n",
    "      </tr>          \n",
    "     <tr>\n",
    "          <td>11</td><td>bewölkt</td><td>hoch</td><td>schwach</td><td>Ja</td>\n",
    "      </tr>          \n",
    "     <tr>\n",
    "          <td>12</td><td>bewölkt</td><td>normal</td><td>schwach</td><td>Ja</td>\n",
    "      </tr>   \n",
    "    </tbody>    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5554710a-7620-4712-a6c9-81398cad89a5",
   "metadata": {},
   "source": [
    "Wir fassen nun die Merkmale als mögliche Ausprägungen eines Zufallsvektors (Merkmalsvektors) $\\underline{X}=(X_1,X_2,X_3)^T$ auf. $X_1, X_2$ und $X_3$ sind also gewöhnliche Zufallsvariablen, die jeweils die folgenden Ausprägungen enthalten:\n",
    "<br><br>\n",
    "$$ X_1  \\in \\left\\{'sonnig','regnerisch','bewölkt'\\right\\}$$\n",
    "$$ X_2  \\in \\left\\{'hoch','normal'\\right\\}$$\n",
    "$$ X_3  \\in \\left\\{'stark','schwach'\\right\\}$$\n",
    "<br><br>\n",
    "Des Weiteren führen wir eine Zufallsvariable für die Ausprägung des Klassenlabels ein: $C \\in \\left\\{'Ja','Nein'\\right\\}$.<br>\n",
    "Wir fragen nun nach der Wahrscheinlichkeit der Ausprägung der Klasse in Abhängigkeit der gegebenen Merkmalsausprägungen:\n",
    "$$P(C=c|\\underline{X}=(x_1,x_2,x_3))$$\n",
    "Es handelt sich hierbei um eine bedingte Wahrscheinlichkeit, also die Wahrscheinlichkeit der Auspägung $c$ des Klassenlabels bei gegebener Ausprägung des Merkmalsvektors. Hier hilft uns nun der Satz von Bayes weiter. Mit seiner Hilfe können wir den Ausdruck wie folgt umformen:<br><br>\n",
    "$$P(C=c|\\underline{X}=(x_1,x_2,x_3))=\\frac{P(\\underline{X}=(x_1,x_2,x_3)|C=c)\\,P(C=c)}{P(\\underline{X}=(x_1,x_2,x_3))}$$<br><br>\n",
    "Die Wahrscheinlichkeit dafür, dass wir einen Ausflug machen, gegeben der Merkmalsvektor $X=('sonnig','normal', 'schwach')$ läßt sich dann  also wie folgt darstellen:<br><br>\n",
    "$$P(C=Ja|\\underline{X}=(sonnig,normal,schwach))=\\frac{P(\\underline{X}=(sonnig,normal,schwach)|C=Ja)\\,P(C=Ja)}{P(\\underline{X}=(sonnig,normal,schwach))}\\hspace{5cm} (*)$$<br><br>\n",
    "Die Wahrscheinlichkeit $P(C=c)$ nennen wir die apriori-Wahrscheinlichkeit für das Auftreten des Klassenlabels $c$. $P(\\underline{X}=(x_1,x_2,x_3))$ ist die Wahrscheinlichkeit für das Auftreten der Merkmalskombination $(x_1,x_2,x_3)$<br>\n",
    "Bis hier hin handelt es sich um einen <i>Bayes-Klassifikator</i>. Die Naivität, die dem naiven Bayes-Klassifikator ihren Namen gibt, besteht nun darin, dass wir von folgender Annahme ausgehen:<br><br>\n",
    "$$P(\\underline{X}=(x_1,x_2,x_3)|C=c) = P(X_1=x_1|C=c)\\cdot P(X_2=x_2|C=c)\\cdot P(X_3=x_3|C=c)$$<br> und somit den folgenden Ausdruck erhalten:\n",
    "$$P(C=c|\\underline{X}=(x_1,x_2,x_3))=\\frac{P(X_1=x_1|C=c)\\cdot P(X_2=x_2|C=c)\\cdot P(X_3=x_3|C=c)\\,P(C=c)}{P(\\underline{X}=(x_1,x_2,x_3))}$$\n",
    "Gilt allgemein $P(A\\cap B|C) = P(A|C)\\cdot P(B|C)$, so sprechen wir von bedingter Unabhängig (conditional independence). Diese <b>darf nicht</b> mit der stochastischen Unabhängigkeit der Ereignisse A und B verwechselt werden, die stattdessen zum Ausdruck bringt, dass $P(A \\cap B) = P(A)\\cdot P(B)$ gilt. Die Naivität besteht in der Praxis nun häufig darin, dass die bedingte Unabhängigkeit häufig gar nicht nachgewiesen, ja zum Teil auch gar nicht streng erfüllt ist. Nichts desto trotz zeigt sich, dass der naive Bayes-Klassifikator dennoch häufig gute Ergebnisse zeigt. Wir funktioniert er nun genau oder anders ausgedrückt, wie berechnen wir nun die Wahrscheinlichkeiten, die in (*) auf der rechten Seite stehen?<br><br>\n",
    "Dies erfolgt unter Verwendung unserer Beispieldaten aus oben stehender Tabelle. Zunächst einmal schätzen wir die apriori-Wahrscheinlichkeit für das Auftreten der beiden möglichen Entscheidungen ab:<br>\n",
    "$$P(C=Ja) \\approx \\frac{8}{12}=\\frac{2}{3} \\mbox{ und } P(C=Nein) \\approx \\frac{4}{12}=\\frac{1}{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8c2d6b-5341-4499-b988-3960002a9b61",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center\"><img src=\"pics/Exercise.png\" alt=\"Bild\" style=\"height: 220px; width: 300px;\"><br></p><br>\n",
    "<b><u>Es ist wieder Zeit für ein bisschen Übung ...</b></u><P>\n",
    "    Schätzen Sie analog die bedingten Wahrscheinlichkeiten $P(X_1=x_1|C=c), P(X_2=x_2|C=c)$ sowie $P(X_3=x_3|C=c)$ basierend auf obiger Tabelle. Wie können Sie $P(\\underline{X}=(x_1,x_2,x_3))$ berechnen, bzw. ist das überhaupt notwendig?<br>\n",
    "    Was ist die wahrscheinlichste Entscheidung für die Merkmalskombination $X=(sonnig, normal, stark)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382db33f-4d6c-4561-b6ec-1232cf21bd90",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <thead>  \n",
    "      <style type=\"text/css\" media=\"screen\">\n",
    "      table, th, td, tr {\n",
    "      padding: 10px;\n",
    "      border: 1px solid #FF0000; \n",
    "      border-collapse: collapse;\n",
    "      }\n",
    "</style>\n",
    "    <tr>\n",
    "      <th colspan=5><b>Aussicht</b></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th></th><th>Ja</th><th>Nein</th><th>Entscheidung=Ja</th><th>Entscheidung=Nein</th>\n",
    "      </tr>    \n",
    "  </thead> \n",
    "  <tbody>\n",
    "      <tr>\n",
    "          <td>sonnig</td><td>?</td><td>?</td><td>?</td><td>?</td>\n",
    "      </tr>    \n",
    "      <tr>\n",
    "          <td>bewölkt</td><td>?</td><td>?</td><td>?</td><td>?</td>\n",
    "      </tr>    \n",
    "     <tr>\n",
    "          <td>regnerisch</td><td>?</td><td>?</td><td>?</td><td>?</td>\n",
    "      </tr>    \n",
    "     <tr>\n",
    "         <td>TOTAL</td><td>?</td><td>?</td><td>100%</td><td>100%</td>\n",
    "      </tr>    \n",
    "      \n",
    "    </tbody>    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b328e9-65e3-42e2-941e-d51344835c04",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <thead>  \n",
    "      <style type=\"text/css\" media=\"screen\">\n",
    "      table, th, td, tr {\n",
    "      padding: 10px;\n",
    "      border: 1px solid #FF0000; \n",
    "      border-collapse: collapse;\n",
    "      }\n",
    "</style>\n",
    "    <tr>\n",
    "      <th colspan=5><b>Luftfeuchtigkeit</b></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th></th><th>Ja</th><th>Nein</th><th>Entscheidung=Ja</th><th>Entscheidung=Nein</th>\n",
    "      </tr>    \n",
    "  </thead> \n",
    "  <tbody>\n",
    "      <tr>\n",
    "          <td>hoch</td><td>?</td><td>?</td><td>?</td><td>?</td>\n",
    "      </tr>    \n",
    "      <tr>\n",
    "          <td>normal</td><td>?</td><td>?</td><td>?</td><td>?</td>\n",
    "      </tr>    \n",
    "     <tr>\n",
    "         <td>TOTAL</td><td>?</td><td>?</td><td>100%</td><td>100%</td>\n",
    "      </tr>    \n",
    "    </tbody>    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6bb220-fa08-4492-a3c2-f8172b2b1719",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <thead>  \n",
    "      <style type=\"text/css\" media=\"screen\">\n",
    "      table, th, td, tr {\n",
    "      padding: 10px;\n",
    "      border: 1px solid #FF0000; \n",
    "      border-collapse: collapse;\n",
    "      }\n",
    "</style>\n",
    "    <tr>\n",
    "      <th colspan=5><b>Windstärke</b></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th></th><th>Ja</th><th>Nein</th><th>Entscheidung=Ja</th><th>Entscheidung=Nein</th>\n",
    "      </tr>    \n",
    "  </thead> \n",
    "  <tbody>\n",
    "      <tr>\n",
    "          <td>stark</td><td>?</td><td>?</td><td>?</td><td>?</td>\n",
    "      </tr>    \n",
    "      <tr>\n",
    "          <td>schwach</td><td>?</td><td>?</td><td>?</td><td>?</td>\n",
    "      </tr>    \n",
    "     <tr>\n",
    "         <td>TOTAL</td><td>?</td><td>?</td><td>100%</td><td>100%</td>\n",
    "      </tr>    \n",
    "    </tbody>    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd147802-a496-4634-93ab-d6afeef29a3e",
   "metadata": {},
   "source": [
    "Worin liegt der Vorteil des naiven Bayes-Klassifikators gegenüber dem Bayes-Klassifikator?<br>\n",
    "Für die Klassifikation werden wir nun nicht umhinkommen als einer Merkmalskombination die Klasse zuzuweisen, die die größte Wahrscheinlichkeit aufweist. Da der Nenner $P(\\underline{X}=(x_1,x_2,x_3)$ bei allen Klassen gleich ist, genügt es folglich den folgenden Ausdruck zu bestimmen:<br><br>\n",
    "$$c^* = arg \\max\\limits_{c} \\left(P(X_1=x_1|C=c)\\cdot P(X_2=x_2|C=c)\\cdot P(X_3=x_3|C=c)\\,P(C=c)\\right)$$<br>\n",
    "\n",
    "Dies ist so zu verstehen, dass wir von allen Klassenlabeln $c$ dasjenige auswählen, welches den Ausdruck in der Klammer maximiert!!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d70b83f-7b96-490a-a694-47c4eab6ee76",
   "metadata": {},
   "source": [
    "Damit haben wir die Grundlagen des naiven Bayes-Klassifikators behandelt. Doch was tun wir, wenn wir statt diskreter Merkmale nun kontinuierliche Merkmale erhalten. In diesem Fall schätzen wir die Wahrscheinlichkeiten basierend auf einer angenommenen Verteilung. Hier kommt häufig, jedoch nicht ausschließlich, die Normalverteilung zur Anwendung. Wir verwenden dann die \"Trainingsdaten\" zur Schätzung der Parameter der gewählten Verteilung. Das weitere Vorgehen bleibt wir hier bereits besprochen. Die Möglichkeit der Auswahl unterschiedlicher Verteilungen erklärt, warum es sich beim naiven Bayes-Klassifikator eigentlich um eine ganze Familie von ähnlichen Klassifikatoren handelt. <br>\n",
    "Wir wollen auch diesen Fall behandeln, bedienen uns dabei jedoch der Bibliothek <i>sklearn</i>. Doch zunächst brauchen wir einen Trainingsdatensatz. Zum Glück unterstützt uns sklearn auch hier und liefert eine Reihe von fertigen Trainingsdatensätzen.  Wir wollen hier und auch für Vergleichszwecke mit noch zu besprechenden Klassifikatoren den so genannten <b>Iris-Datensatz</b> verwenden. Es handelt sich hierbei um einen klassischen Multiklassen-Datensatz (3 Klassen, 50 Beispiele pro Klasse, 4 reelle Merkmale). Es werden drei verschiedene Arten von Schwertlilien (Iris Setosa, Iris Virginica und Iris Versicolor) anhand Merkmale Länge und Breite von Kelchblatt und Kronblatt unterschieden.  <br><br>\n",
    "<p style=\"text-align: center\"><img src=\"pics/Iris_versicolor.jpg\" alt=\"Bild\" style=\"height: 110px; width: 150px;\">\n",
    "                              <img src=\"pics/Iris_virginica.jpg\" alt=\"Bild\" style=\"height: 110px; width: 150px;\">\n",
    "                              <img src=\"pics/Iris_setosa.jpg\" alt=\"Bild\" style=\"height: 150px; width: 100px;\"><br>Quelle: Wikipedia</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82114fd9-280e-43e8-8759-1730e75466f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# load iris dataset\n",
    "data = load_iris()\n",
    "\n",
    "# separate features from class labels\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# data splitting (training and test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.5, random_state=42)\n",
    "\n",
    "# create model\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# use model for predicting test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# choose metric and evaluate the model's performance\n",
    "print(\"Naive Bayes Classifiers accuracy:\", metrics.accuracy_score(y_test, y_pred)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07768ad1-90f8-4189-8ae9-c07d6580a9d9",
   "metadata": {},
   "source": [
    "Weiteres interessantes Material zu diesem Kapitel ist zu finden unter<br>\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes\">sklearn.naive_bayes</a><br>\n",
    "<a href=\"https://de.wikipedia.org/wiki/Bayesscher_Spamfilter\">Bayesscher Spamfilter</a><br>\n",
    "<a href=\"https://www.probabilitycourse.com/chapter1/1_4_4_conditional_independence.php\">Bedingte Unabhängigkeit</a><br>\n",
    "<a href=\"https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets\">Datensätze in sklearn</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f54ea1f-ce8c-468a-901e-7154c9b7224c",
   "metadata": {},
   "source": [
    "<a id=\"s2\"></a><h3 style=\"color:rgb(127,203,223)\">3.2 Die Logistische Regression</h3>\n",
    "Bei der Logistischen Regression handelt es sich, wie der Name bereits suggeriert, um ein Regressionverfahren. Da hier jedoch der Regressand (die Ausgabegröße) eine Wahrscheinlichkeit darstellt, kann dieses Verfahren durch anschließende Bestimmung eines Maximums auch als Klassifikationsverfahren eingesetzt werden.<br>\n",
    "Wir wollen zunächst mit der binomialen Logistischen Regression beginnen. Hierbei sei $Y$ eine Zufallsvariable, die zwei unterschiedliche Realisierungen annehmen kann (o.B.d.A. wählen wir hierfür 0 und 1). Wir interessieren uns nun für die bedingte Wahrscheinlichkeit $P(Y=y|\\underline{X}=(x_1,...,x_n))$ wobei $\\underline{X}$ wieder ein Zufallsvektor sein soll, der unseren Merkmalsvektor darstellt. Eine direkte Darstellung der Wahrscheinlichkeit in der Form $P(Y=y|\\underline{X}=(x_1,\\dots,x_n))=\\beta_0+\\beta_1\\, x_1 + \\dots + \\beta_n \\, x_n$ mach hierbei keinen Sinn, da im allgemeinen nicht nur Zahlen zwischen Null und Eins sondern beliebige Zahlen herauskommen würden, was die Interpretation als Wahrscheinlichkeitswert verbieten würde. Wir betrachten stattdessen die so genannte Chance (engl. odds), also das Verhältnis von $P(Y=1)$ zur Gegenwahrscheinlichkeit $P(Y=0)=1-P(Y=1)$. Wir definieren folglich $odds = \\frac{P(Y=1)}{1-P(Y=1)}$. Wie sieht der mögliche Wertebereich aus? Es gilt $odds \\in ]0,\\infty[$. Auch hier macht ein Ansatz der Form <br>\n",
    "$$odds = \\beta_0+\\beta_1\\, x_1 + \\dots + \\beta_n \\, x_n$$ <br> keinen Sinn, da auf der rechten Seite natürlich im allgemeinen auch negative Werte auftauchen können. Welche Funktion kennen Sie, die das Intervall $]0,\\infty[$ in das Intervall $]-\\infty,\\infty[$ abbildet?<br>\n",
    "Richtig: Wir verwenden die Logarithmusfunktion. Wenn wir diese auf die odds anwenden, so erhalten wir den so genannten <i>Logit</i> also den natürlichen Logarithmus der Chance. Es gilt folglich:<br><br>\n",
    "$$Logit = \\ln(odss)=\\ln\\left(\\frac{P(Y=1)}{1-P(Y=1)}\\right)$$<br>\n",
    "Unter Verwendung des Logit macht der folgende Ansatz nun Sinn:<br><br>\n",
    "$$Logit(Y=y|\\underline{X}=(x_1,\\dots,x_n))=\\beta_0+\\beta_1\\, x_1 + \\dots + \\beta_n \\, x_n$$<br>\n",
    "Durch die Anwendung der Exponentialfunktion auf beiden Seiten erhalten wir dann:<br><br>\n",
    "$$\\frac{P(Y=1|\\underline{X}=(x_1,\\dots,x_n))}{1-P(Y=1|\\underline{X}=(x_1,\\dots,x_n))}=\\exp(\\beta_0+\\beta_1\\, x_1 + \\dots + \\beta_n \\, x_n)$$<br>\n",
    "Diese Gleichung können wir nun nach $P(Y=1|\\underline{X}=(x_1,\\dots,x_n))$ auflösen und erhalten das (binomiale) logistische Regressionsmodell<br><br>\n",
    "$$P(Y=1|\\underline{X}=(x_1,\\dots,x_n))=\\frac{\\exp(\\beta_0+\\beta_1\\, x_1 + \\dots + \\beta_n \\, x_n)}{1+\\exp(\\beta_0+\\beta_1\\, x_1 + \\dots + \\beta_n \\, x_n)}$$<br>\n",
    "Im Gegensatz zur linearen Regression ist eine direkte Berechnung der unbekannten Parameter $\\beta_0,\\dots,\\beta_n$ nicht möglich. Stattdessen kommen numerische Verfahren zum Einsatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5c402-897f-4c55-ab54-644a247801af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='https://www.geogebra.org/m/yedav6gf', width=800, height=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d182df-e97d-4119-9ce0-6a33af403693",
   "metadata": {},
   "source": [
    "In dem wir uns nach der Berechnung der Wahrscheinlichkeiten für die Realisierung von $Y$ entscheiden, die die größte Wahrscheinlichkeit aufweist, können wir also die Logistische Regression auch direkt zu Klassifikation verwenden. Bisher ist jedoch lediglich eine binäre Klassifikation möglich. Wir müssen uns also anschauen, wir wir den bisherigen Ansatz auf den Fall einer Multiklassen-Klassifikation erweitern können. Abschliessend werden wir uns dann nochmals dem Iris-Datensatz zuwenden und schauen, welche Accuracy wir unter Verwendung der logistischen Regression erreichen.<br>\n",
    "Die Erweiterung der logistischen Regression auf den Multiklassen-Fall ist auch unter der Bezeichnung <i>Multinomiale logistische Regression</i> bekannt. Die Idee der Erweiterung der logistischen Regression auf eine Zufallsvariable $Y$ mit mehr als zwei möglichen Realisierungen erfolgt dabei folgendermaßen:<br>\n",
    "Gehen wir davon aus, dass es $K$ mögliche Ausprägungen (Klassen $1,\\dots,K$) von $Y$ gibt. Wir können dann eine Ausprägung (Klasse) auswählen, bspw. die Klasse $K$. Nun setzen wir wie folgt an:<br><br>\n",
    "$$Logit = \\ln\\left(\\frac{P(Y=1|\\underline{X}=(x_1,...,x_n))}{P(Y=K|\\underline{X}=(x_1,...,x_n))}\\right) = \\beta_{01}+\\beta_{11}\\, x_1 + \\dots + \\beta_{n1} \\, x_n$$<br>\n",
    "$$Logit = \\ln\\left(\\frac{P(Y=2|\\underline{X}=(x_1,...,x_n))}{P(Y=K|\\underline{X}=(x_1,...,x_n))}\\right) = \\beta_{02}+\\beta_{12}\\, x_1 + \\dots + \\beta_{n2} \\, x_n$$<br>\n",
    "$$\\vdots$$<br>\n",
    "$$Logit = \\ln\\left(\\frac{P(Y=K-1|\\underline{X}=(x_1,...,x_n))}{P(Y=K|\\underline{X}=(x_1,...,x_n))}\\right) = \\beta_{0K-1}+\\beta_{1K-1}\\, x_1 + \\dots + \\beta_{nK-1} \\, x_n$$<br><br>\n",
    "Durch exponentieren beider Seiten folgt dann:<br><br>\n",
    "$$P(Y=1|\\underline{X}=(x_1,...,x_n)) = P(Y=K|\\underline{X}=(x_1,...,x_n))\\, \\exp(\\beta_{01}+\\beta_{11}\\, x_1 + \\dots + \\beta_{n1} \\, x_n)$$<br>\n",
    "$$\\vdots$$<br>\n",
    "$$P(Y=K-1|\\underline{X}=(x_1,...,x_n)) = P(Y=K|\\underline{X}=(x_1,...,x_n))\\, \\exp(\\beta_{0K-1}+\\beta_{1K-1}\\, x_1 + \\dots + \\beta_{nK-1} \\, x_n)$$<br><br>\n",
    "Da sich alle Wahrscheinlichkeiten zu Eins aufaddieren müssen, gilt:<br><br>\n",
    "$$P(Y=K|\\underline{X}=(x_1,...,x_n))=1-\\sum\\limits_{k=1}^{K-1}P(Y=k|\\underline{X}=(x_1,...,x_n))=1-\\sum\\limits_{k=1}^{K-1}P(Y=K|\\underline{X}=(x_1,...,x_n))\\, \\exp(\\beta_{0k}+\\beta_{1k}\\, x_1 + \\dots + \\beta_{nk} \\, x_n))$$<br><br>\n",
    "Hieraus folgt nun unmittelbar:<br><br>\n",
    "$$P(Y=K|\\underline{X}=(x_1,...,x_n))=\\frac{1}{1+\\sum\\limits_{k=1}^{K-1}\\exp(\\beta_{0k}+\\beta_{1k}\\, x_1 + \\dots + \\beta_{nk} \\, x_n)}$$<br>\n",
    "$$P(Y=1|\\underline{X}=(x_1,...,x_n))=\\frac{\\exp(\\beta_{01}+\\beta_{11}\\, x_1 + \\dots + \\beta_{n1} \\, x_n)}{1+\\sum\\limits_{k=1}^{K-1}\\exp(\\beta_{0k}+\\beta_{1k}\\, x_1 + \\dots + \\beta_{nk} \\, x_n)}$$<br>\n",
    "$$\\vdots$$<br>\n",
    "$$P(Y=K-1|\\underline{X}=(x_1,...,x_n))=\\frac{\\exp(\\beta_{0K-1}+\\beta_{1K-1}\\, x_1 + \\dots + \\beta_{nK-1} \\, x_n)}{1+\\sum\\limits_{k=1}^{K-1}\\exp(\\beta_{0k}+\\beta_{1k}\\, x_1 + \\dots + \\beta_{nk} \\, x_n)}$$<br><br>\n",
    "Auch hier würden wir, analog zum naiven Bayes-Klassifikator, wieder die Klasse auswählen, die zur größten Wahrscheinlichkeit führt:<br><br>\n",
    "$$k^* = arg \\max\\limits_{k} P(Y=k|\\underline{X}=(x_1,...,x_n))$$<br>\n",
    "Kommen wir nun noch zum Beispiel des Iris-Datensatzes zurück.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be1e88c-3dab-4f43-bcc0-e957a8c22a92",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center\"><img src=\"pics/Exercise.png\" alt=\"Bild\" style=\"height: 220px; width: 300px;\"><br></p><br>\n",
    "<b><u>Es ist wieder Zeit für ein bisschen Übung ...</b></u><P>\n",
    "    Verwenden Sie sklearn um basierend auf dem Ihnen nun schon bekannten Iris-Datensatz die Multinomiale logistische Regression anzuwenden. Vergleichen Sie auch mit den Ergebnissen des naiven Bayes-Klassifikators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3423f8-bd38-42ca-a8f0-77ee2f40d8a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load iris dataset\n",
    "data = load_iris()\n",
    "\n",
    "# separate features from class labels\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# data splitting (training and test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.4)\n",
    "\n",
    "# create model\n",
    "model = LogisticRegression(max_iter=500\n",
    "                          )\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# use model for predicting test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# choose metric and evaluate the model's performance\n",
    "\n",
    "\n",
    "print(\"Logistic Regression accuracy:\", metrics.accuracy_score(y_test, y_pred)*100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6420b2-08c3-4162-8b1f-9ccd0893c901",
   "metadata": {},
   "source": [
    "Weiteres interessantes Material zu diesem Kapitel ist zu finden unter<br>\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">sklearn.linear_model.LogisticRegression</a><br>\n",
    "<a href=\"https://en.wikipedia.org/wiki/Logistic_regression\">Logistic Regression</a><br>\n",
    "<a href=\"https://en.wikipedia.org/wiki/Multinomial_logistic_regression\">Multinomial logistic regression</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0bb614-f039-48ea-bc08-9da88a4dec08",
   "metadata": {},
   "source": [
    "<a id=\"s3\"></a><h3 style=\"color:rgb(127,203,223)\">3.3 Classification and Regression Trees CART</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32daf4a-4cf7-4075-9cc5-ebdff9a7d687",
   "metadata": {},
   "source": [
    "Klassifikations- und Regressions-Bäume stellen eine einheitliche Datenstruktur einschließlich zugehöriger Algorithmen dar um Klassifikations- und Regressionsprobleme zu lösen. Der Begriff (<b>C</b>lassification <b>A</b>nd <b>R</b>egression <b>T</b>rees) geht hierbei auf <i>Leo Breiman</i> zurück. Ein großer Vorteil besteht, wie wir noch sehen werden, darin, dass die Entscheidungen im einzelnen zumindest prinzipiell nachvollziehbar sind. <br>\n",
    "Wir beginnen mit der Betrachtung eines einfachen Entscheidungsbaumes um die grundsätzliche Struktur sowie die verwendeten Algorithmen zum Aufbau eines Entscheidungsbaumes kennenzulernen. Bei einem Entscheidungsbaum handelt es sich mathematisch betrachtet um einen Graphen, der die Struktur eines Baumes besitzt. Ein Baum besteht aus Knoten, von denen jeweils zwei mit einer Kante verbunden sein können.  Der Baum beginnt mit einem Wurzelknoten, der mit einem oder mehreren (Kind-)Knoten verbunden ist. Diese wiederum können ebenfalls wieder mit einem oder mehreren (Kind-)Knoten verbunden sein. Diese Struktur setzt sich fort bis zu den Knoten, die über keine Kind-Knoten mehr verfügen. Diese Knoten werden auch Blätter genannt.<br><br>\n",
    "<p style=\"text-align: center\"><img src=\"pics/Baum.png\" alt=\"Bild\" style=\"height: 263px; width: 300px;\"><center>Quelle: Wikipedia</center><br></p><br>\n",
    "Bei obigem Beispiel ist also der obere Knoten mit der Nummer 2 der Wurzelknoten (root node) und der Knoten mit der Nummer 5 wäre ein Blatt (leaf). Bäume werden üblicherweise mit der Wurzel an von oben nach unten gezeichnet.<br>\n",
    "Einem fertig trainierten Entscheidungsbaum wird nun ein Merkmalsvektor an der Wurzel präsentiert. Jeder Knoten, mit Ausnahme der Blätter, wird nun eine Entscheidung basierend auf einem Merkmal treffen und die verbliebenen Merkmale in Abhängigkeit dieser Entscheidung an einen seiner Kind-Knoten weiterleiten. So wird nach einer endlichen Anzahl von Entscheidungen schließlich ein Blatt erreicht. Die Blätter repräsentieren hierbei die möglichen Klassen. Der Baum votiert folglich für die Klasse, zu der das finale Blatt gehört. Schauen wir uns das Ganze einmal an einem einfachen Beispiel an:<br><br>\n",
    "<p style=\"text-align: center\"><img src=\"pics/Entscheidungsbaum.png\" alt=\"Bild\" style=\"height: 350px; width: 500px;\"><center>Einfacher Entscheidungsbaum</center><br></p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1195afb5-a2d0-423e-9c6e-392d4ec75904",
   "metadata": {},
   "source": [
    "Wie bauen wir nun einen solchen Baum auf? Es ist ja nicht naheliegend, mit welchen Merkmalen wir beginnen. Um ein mögliches Vorgehen, es gibt hier in der Tat unterschiedliche Algorithmen, zu demonstrieren, wollen wir wieder auf die <a href=\"Chapter03.ipynb#s4\">Daten</a> zurückgreifen, die wir bereits für den naiven Bayes-Klassifikator verwenden haben.<br>\n",
    "Ziel ist es, die Trainigsdaten an jedem Entscheidungsknoten so zu trennen, dass nach Möglichkeit im Hinblick auf das Klassenlabel homogene Teilmengen entstehen, die dann von den Kindknoten weiterverarbeitet werden können. Sollten sich in dieser Teilmenge nur noch Daten einer Klasse befinden, so wird aus dem zugehörigen Kind-Knoten automatisch ein Blatt, dass mit dem entsprechenden Klassenlabel attributiert wird. <br> Wir werden nun einen Algorithmus kennenlernen, der es uns ermöglicht, gute Aufteilungen (Splits) an einem Knoten vorzunehmen. Ziel soll ein Entscheidungsbaum sein, der in der Lage ist, die Trainingsdaten möglichst perfekt zu klassifizieren. Einige andere zusätzlich wünschenswerte Eigenschaften werden wir anschließend diskutieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7746626-92aa-4809-ae52-c00d193f32b0",
   "metadata": {},
   "source": [
    "Zunächst einmal benötigen wir eine Metrik, um die \"Reinheit\" (purity) einer Aufteilung zu beschreiben. Anhand dieser Metrik werden wir anschließend \"alle\" möglichen Aufteilungen untersuchen und die im aktuellen Knoten verwenden, die die größte Reinheit der so aufgeteilten Daten zur Folge hat.<br>\n",
    "Wir können für den Trainingsdatensatz die apriori-Wahrscheinlichkeit für die Entscheidung \"Ja\" bzw. \"Nein\" aus der Tabelle ablesen:<br><br>\n",
    "$$P(Y=\\mbox{'Ja'})=\\frac{8}{12}=\\frac{2}{3} \\mbox{ und } P(Y=\\mbox{'Nein'})=\\frac{4}{12}=\\frac{1}{3}$$<br>\n",
    "Hierbei entspricht $Y$ der Entscheidung, ob ein Ausflug stattfinden soll oder nicht. Ein mögliches Maß für die Reinheit der Daten (im Hinblick auf die Klassenverteilung) ist die so genannte Entropie. Diese ist wie folgt definiert:<br><br>\n",
    "$$H(Y)=-P(Y=\\mbox{'Ja'})\\,\\ln_2(P(Y=\\mbox{'Ja'}))-P(Y=\\mbox{'Nein'})\\,\\ln_2(P(Y=\\mbox{'Nein'}))$$<br>\n",
    "Wenn wir $P(Y=\\mbox{'Ja'})=x$ setzen und berücksichtigen, dass $P(Y=\\mbox{'Nein'})=1-x$ gilt, so können wir die Entropie einfach graphisch darstellen:\n",
    "<p style=\"text-align: center\"><img src=\"pics/Entropy.png\" alt=\"Bild\" style=\"height: 350px; width: 500px;\"><center>Entropie</center><br></p><br>\n",
    "Die Entropie wird maximal für den Fall einer Gleichverteilung und Null für den Fall vollständiger Reinheit (Alle Daten gehören einer Klasse an). Wir können somit die Entropie der Daten im aktuellen Knoten bestimmen. Nachdem wir nun eine Aufteilung vorgenommen haben, dazu gleich mehr, können wir die Entropie ebenso für die beiden Kind-Knoten berechnen. Betrachten wir das Ganze mal anhand eines Beispiels:<br><br>\n",
    "Nehmen wir an, wir nehmen eine Aufteilung vor anhand des Merkmals \"Aussicht\". Wir teilen die Trainingsdaten in zwei Teilmengen auf und zwar sollen alle Trainingsdaten, deren Merkmal Aussicht = \"bewölkt\" ist in eine Menge (entspricht dann einem Kind-Knoten) und alle anderen in eine andere Menge (entspricht dem anderen Kind-Knoten) aufgeteilt werden. Berechnen wir zunächst die ursprüngliche Entropie anhand aller Daten:<br><br>\n",
    "$$H(Y)=-P(Y=\\mbox{'Ja'})\\,\\ln_2(P(Y=\\mbox{'Ja'}))-P(Y=\\mbox{'Nein'})\\,\\ln_2(P(Y=\\mbox{'Nein'})) $$ $$= -\\frac{2}{3}\\,\\ln_2(\\frac{2}{3})-\\frac{1}{3}\\,\\ln_2(\\frac{1}{3})\\approx 0.918$$<br>\n",
    "Nun teilen wir die Daten gemäß des obigen Kriteriums. Wir erhalten dabei die beiden folgenden Mengen:<br><br>\n",
    "$$M_1=\\left\\{1,2,3,4,7,8,9,10\\right\\}$$\n",
    "$$M_2=\\left\\{5, 6, 11,12 \\right\\}$$<br><br>\n",
    "hierbei sind jeweils die Indizes der einzelnen Merkmalsvektoren (siehe erste Spalte in der Datentabelle) dargestellt. Wir können nun auch die Entropien dieser beiden Datensätze berechnen:<br><br>\n",
    "$$H(Y|M_1)=-\\frac{4}{8}\\,\\ln_2(\\frac{4}{8})-\\frac{4}{8}\\,\\ln_2(\\frac{4}{8})=1$$<br>\n",
    "$$H(Y|M_2)=-\\frac{4}{4}\\,\\ln_2(\\frac{4}{4})-\\frac{0}{4}\\,\\ln_2(\\frac{0}{4})=0$$<br><br>\n",
    "Wir können nun den so genannten <b>Information Gain IG</b> berechnen. Dieser gibt an, um wie viel die Entropie sich verringert hat. Der Information Gain ist wie folgt definiert:<br><br>\n",
    "$$IG=H(Y)-\\left(\\frac{8}{12}\\,H(Y|M_1)+\\frac{4}{12}\\,H(Y|M_2)\\right)$$<br><br>\n",
    "Da das Ziel ist, die Entropie nach der Aufteilung zu verringern, besteht das Vorgehen darin, den Information Gain zu maximieren. Wir erhalten für unser Beispiel:<br><br>\n",
    "$$IG = 0.918 - (\\frac{2}{3}\\,1 + \\frac{1}{3}\\,0) \\approx 0.251$$<br><br>\n",
    "Sie können leicht nachrechnen, indem Sie die Aufteilung nach den anderen beiden Merkmalsausprägungen (\"regnerisch\",\"sonnig\") durchführen, dass dieser Information Gain maximal ist. Das gilt auch für den Vergleich mit den anderen beiden Merkmalen. Während des Trainings werden in diesem Beispiel alle Varianten analysiert und die Aufteilung erfolgt dann gemäß der Variante, die den Information Gain maximiert oder gleichbedeutend die Reinheit der neuen Mengen hinsichtlich der Klassenverteilung maximiert.<br>\n",
    "Das Verfahren wird nun rekursiv auf den verbliebenen Mengen fortgeführt. Sobald eine Menge rein ist, also nur noch Beispiele einer Klasse enthält, wird aus diesem Knoten ein Blatt. Liegen nur noch Blätter vor, so ist der Algorithmus beendet. Alternativ dazu kann auch noch eine maximale Tiefe des Baumes oder eine minimale Anzahl von Elementen pro Menge als Abbruchkriterium des Algorithmus vorgegeben werden. Wenden wir nun die entsprechende Funktionalität in <i>sklearn</i> an, um den Entscheidungsbaum berechnen zu lassen. Hierbei muss jedoch beachtet werden, dass sklearn leider im Zusammenhang mit Entscheidungsbäumen nicht mit kategorischen Variablen arbeiten kann. Daher verwenden wir die folgende Übersetzung:<br><br>\n",
    "$$\\mbox{Aussicht: sonnig=0, regnerisch=1, bewölkt=2}$$\n",
    "$$\\mbox{Luftfeuchtigkeit: hoch=0, normal=1}$$\n",
    "$$\\mbox{Windstärke: schwach=0, stark=1}$$<br><br>\n",
    "sklearn wird diese Merkmale als kontinuierlich statt als diskret auffassen. Das muss beachtet werden, führt aber nicht zu Einbußen beim Training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b72c6ae-6ac3-4a48-bc7f-c060c55bcdd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "\n",
    "# Merkmale (Aussicht, Luftfeuchtigkeit, Windstärke)\n",
    "sonnig=0\n",
    "regnerisch=1\n",
    "bewoelkt=2\n",
    "hoch=0\n",
    "normal=1\n",
    "schwach=0\n",
    "stark=1\n",
    "nein=0\n",
    "ja=1\n",
    "\n",
    "X = [[sonnig,hoch,schwach],[sonnig,normal,stark],[regnerisch, hoch, schwach],[bewoelkt, normal,schwach],[bewoelkt, normal, stark],[regnerisch, hoch, stark]]\n",
    "\n",
    "# zugehörige Klassenlabel (nein=0 und ja=1)\n",
    "Y = [nein, ja,ja, ja,ja,nein]\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf = clf.fit(X, Y)\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None,\n",
    "    feature_names=['Aussicht','Luftfeuchtigkeit','Windstärke'],\n",
    "    class_names=['Nein','Ja'],filled=True, rounded=True,special_characters=True)\n",
    "\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214af9f7-b681-43db-be57-0e08d97e6c68",
   "metadata": {
    "tags": []
   },
   "source": [
    "  Neben der Entropie wird auch die so genannte <i>Gini Impurity</i> als Maß für die Reinheit einer Teilung verwendet (Beide Varianten werden in sklearn unterstützt). Die Gini Impurity ist dabei wie folgt definiert:<br><br>\n",
    "$$G = \\sum\\limits_{i=1}^J \\left( p_i\\, \\left(\\sum\\limits_{k \\neq i} p_k \\right)\\right)=\\sum\\limits_{i=1}^J p_i\\,(1-p_i)=\\sum\\limits_{i=1}^J p_i-p_i^2 = 1 - \\sum\\limits_{i=1}^J p_i^2$$<br><br>\n",
    "Hierbei sei $J$ die Anzahl der unterschiedlichen Klassenlabel und die $p_i$ entsprechen den relativen Häufigkeiten der einzelnen Klassen im jeweiligen Datensatz. Wählen wir zufällig ein Datum aus, so erhalten wir mit der Wahrscheinlichkeit $p_i$ ein Klassenlabel der Klasse $i$. Die Wahrscheinlichkeit, dass dieses Datum fälschlicherweise als eine der anderen Klassen klassifiziert wird, wenn wir ein Klassenlabel zufällig wählen (jedoch unter Berücksichtigung der Häufigkeit im Datensatz), beträgt dann $p_i \\, (1-p_i)$. Die Gesamtwahrscheinlichkeit der Fehlklassifikation eines Datums mit beliebigem Klassenlabel entspricht dann der Gini Impurity. Die Gini Impurity nimmt ihr Minimum von Null an im Falle eines reinen Datensatzes.     \n",
    "Wir haben die Gini Impurity hier direkt für den allgemeinen Fall von $J$ Klassen definiert. Auch der weiter oben eingeführte Information Gain läßt sich einfach auf den Fall von $J$ Klassen erweitern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45478fdf-bc2c-43f8-9dc9-b90e6b84025e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style=\"text-align: center\"><img src=\"pics/Exercise.png\" alt=\"Bild\" style=\"height: 220px; width: 300px;\"><br></p><br>\n",
    "<b><u>Es ist wieder Zeit für ein bisschen Übung ...</b></u><P>\n",
    "    Verwenden Sie die Merkmale und Klassenlabel aus der folgenden Tabelle und erstellen Sie händisch einen Entscheidungsbaum. Sie können hierbei frei wählen, ob Sie den Information Gain oder die Gini Impurity verwenden wollen. Nachdem Sie den Baum händisch erstellt haben, verifizieren Sie ihr Ergebnis unter Verwendung von sklearn. <br><br>\n",
    "    <table>\n",
    "  <thead>  \n",
    "      <tr>\n",
    "          <th>Index</th><th>Merkmal 1</th><th>Merkmal 2</th>\n",
    "          <th>Klassenlabel</th>\n",
    "      </tr>\n",
    "    </thead> \n",
    "    <tbody>\n",
    "      <tr>\n",
    "          <td>1</td><td>0</td><td>0</td><td>0</td>\n",
    "      </tr>  \n",
    "     <tr>\n",
    "          <td>2</td><td>0</td><td>1</td><td>0</td>\n",
    "      </tr>                  \n",
    "     <tr>\n",
    "         <td>3</td><td>1</td><td>1</td><td>1</td>\n",
    "      </tr>  \n",
    "      <tr>\n",
    "          <td>4</td><td>2</td><td>0</td><td>0</td>\n",
    "      </tr>  \n",
    "     <tr>\n",
    "          <td>5</td><td>2</td><td>1</td><td>0</td>\n",
    "      </tr>          \n",
    "     <tr>\n",
    "          <td>6</td><td>1</td><td>0</td><td>1</td>\n",
    "      </tr>  \n",
    "    </tbody>    \n",
    "</table><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a86e28a-59a0-4ffa-9a43-b97dac665be9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 9.0.0 (20230911.1827)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"278pt\" height=\"269pt\"\n",
       " viewBox=\"0.00 0.00 278.00 269.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 265)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-265 274,-265 274,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<path fill=\"#f2c09c\" stroke=\"black\" d=\"M150.25,-261C150.25,-261 63.75,-261 63.75,-261 57.75,-261 51.75,-255 51.75,-249 51.75,-249 51.75,-205 51.75,-205 51.75,-199 57.75,-193 63.75,-193 63.75,-193 150.25,-193 150.25,-193 156.25,-193 162.25,-199 162.25,-205 162.25,-205 162.25,-249 162.25,-249 162.25,-255 156.25,-261 150.25,-261\"/>\n",
       "<text text-anchor=\"start\" x=\"59.75\" y=\"-243.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Merkmal1 ≤ 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"71.75\" y=\"-228.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n",
       "<text text-anchor=\"start\" x=\"69.88\" y=\"-213.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n",
       "<text text-anchor=\"start\" x=\"68\" y=\"-198.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [4, 2]</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M82,-149.5C82,-149.5 12,-149.5 12,-149.5 6,-149.5 0,-143.5 0,-137.5 0,-137.5 0,-108.5 0,-108.5 0,-102.5 6,-96.5 12,-96.5 12,-96.5 82,-96.5 82,-96.5 88,-96.5 94,-102.5 94,-108.5 94,-108.5 94,-137.5 94,-137.5 94,-143.5 88,-149.5 82,-149.5\"/>\n",
       "<text text-anchor=\"start\" x=\"19.25\" y=\"-132.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"9.88\" y=\"-117.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-102.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [2, 0]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M87.35,-192.6C81.14,-182.03 74.25,-170.32 67.94,-159.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"70.98,-157.86 62.89,-151.01 64.94,-161.4 70.98,-157.86\"/>\n",
       "<text text-anchor=\"middle\" x=\"55.85\" y=\"-168.11\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M210.25,-157C210.25,-157 123.75,-157 123.75,-157 117.75,-157 111.75,-151 111.75,-145 111.75,-145 111.75,-101 111.75,-101 111.75,-95 117.75,-89 123.75,-89 123.75,-89 210.25,-89 210.25,-89 216.25,-89 222.25,-95 222.25,-101 222.25,-101 222.25,-145 222.25,-145 222.25,-151 216.25,-157 210.25,-157\"/>\n",
       "<text text-anchor=\"start\" x=\"119.75\" y=\"-139.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Merkmal1 ≤ 1.5</text>\n",
       "<text text-anchor=\"start\" x=\"139.25\" y=\"-124.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"129.88\" y=\"-109.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 4</text>\n",
       "<text text-anchor=\"start\" x=\"128\" y=\"-94.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [2, 2]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M126.65,-192.6C131.41,-184.5 136.57,-175.72 141.56,-167.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"144.49,-169.17 146.54,-158.78 138.46,-165.62 144.49,-169.17\"/>\n",
       "<text text-anchor=\"middle\" x=\"153.58\" y=\"-175.87\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<path fill=\"#399de5\" stroke=\"black\" d=\"M146,-53C146,-53 76,-53 76,-53 70,-53 64,-47 64,-41 64,-41 64,-12 64,-12 64,-6 70,0 76,0 76,0 146,0 146,0 152,0 158,-6 158,-12 158,-12 158,-41 158,-41 158,-47 152,-53 146,-53\"/>\n",
       "<text text-anchor=\"start\" x=\"83.25\" y=\"-35.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"73.88\" y=\"-20.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n",
       "<text text-anchor=\"start\" x=\"72\" y=\"-5.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M147.26,-88.68C142.38,-80.45 137.13,-71.59 132.18,-63.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"135.23,-61.51 127.12,-54.7 129.2,-65.08 135.23,-61.51\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M258,-53C258,-53 188,-53 188,-53 182,-53 176,-47 176,-41 176,-41 176,-12 176,-12 176,-6 182,0 188,0 188,0 258,0 258,0 264,0 270,-6 270,-12 270,-12 270,-41 270,-41 270,-47 264,-53 258,-53\"/>\n",
       "<text text-anchor=\"start\" x=\"195.25\" y=\"-35.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"185.88\" y=\"-20.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n",
       "<text text-anchor=\"start\" x=\"184\" y=\"-5.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [2, 0]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M186.74,-88.68C191.62,-80.45 196.87,-71.59 201.82,-63.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"204.8,-65.08 206.88,-54.7 198.77,-61.51 204.8,-65.08\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x1240cb490>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "# Merkmale (Aussicht, Luftfeuchtigkeit, Windstärke)\n",
    "X = [[0,0],[0,1],[1,1],[2,0],[2,1],[1,0]]\n",
    "\n",
    "# zugehörige Klassenlabel (nein=0 und ja=1)\n",
    "Y = [0,0,1,0,0,1]\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion='gini',max_features=2,splitter='best')\n",
    "clf = clf.fit(X, Y)\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None,\n",
    "    feature_names=['Merkmal1','Merkmal2'],\n",
    "    filled=True, rounded=True,special_characters=True)\n",
    "\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af246f9-1b46-4ac7-a9f6-98490e74e67e",
   "metadata": {},
   "source": [
    "Was ändert sich, wenn wir nun statt kategorialen Merkmalen kontinuierliche Merkmale verwenden?<br>\n",
    "Nun eine Aufteilung basierend auf einen konkreten Wert eines Merkmals macht nun keinen Sinn mehr. Stattdessen verwenden wir Schwellwerte. Wir teilen den Datensatz unter Verwendung eines ausgwählten Merkmales danach auf, ob die Realisierung des Merkmals größer oder kleiner-gleich einem gegeben Schwellwert ist. Die Schwellwerte können wir dabei entweder ähnlich zum Vorgehen bei der Receiver Operating Characteristic bestimmen oder - was besonders bei einer sehr großen Zahl von Beispieldaten sinnvoll ist - wir führen ein <i>Clustering</i> durch. Hierbei werden die Daten anhand einer Metrik zu Clustern zusammengefasst. Wir müssen nun die Schwellwerte lediglich so setzen, dass sie zwischen den Clusterzentren liegen. <br>\n",
    "Das Verfahren lässt sich direkt auf den Fall der Multiklassen-Klassifikation übertragen. Hierbei wird analog vorgegangen und der Information Gain bzw. die Gini Impurity für den Fall von $J$ Klassen verwendet. Es soll hier noch angemerkt werden, dass es eine Reihe von Varianten bei Entscheidungsbäumen gibt. Beispielsweise muss nicht zwingend ein Binärbaum entstehen. Des Weiteren gibt es auch Varianten bei der Bestimmung der Aufteilung. Erwähnt werden sollen in diesem Zusammenhang die Algorithmen <b>ID3</b> (Iterative Dichotomiser 3), <b>C4.5</b> und <b>C5.0</b>. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bba125d-7a9f-4287-abd2-79b045c7e2c7",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center\"><img src=\"pics/Exercise.png\" alt=\"Bild\" style=\"height: 220px; width: 300px;\"><br></p><br>\n",
    "<b><u>Es ist wieder Zeit für ein bisschen Übung ...</b></u><P>\n",
    "    Verwenden Sie den bereits bekannten Iris-Datensatz sowie die entsprechende Funktionalität in sklearn um einen Entscheidungsbaum zu erstellen. Denken Sie daran, die Daten zuvor wieder in Trainings-, Validierungs- und Testdaten aufzuteilen.<br>\n",
    "    <a href=\"https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html#sphx-glr-auto-examples-tree-plot-iris-dtc-py\">Plot the decision surface of decision trees trained on the iris dataset</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d5b65b1-cb70-445a-b169-1eb080118d40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classification accuracy: 100.0\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 11.0.0 (20240428.1522)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"768pt\" height=\"561pt\"\n",
       " viewBox=\"0.00 0.00 767.50 561.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 557)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-557 763.5,-557 763.5,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M459,-553C459,-553 351.5,-553 351.5,-553 345.5,-553 339.5,-547 339.5,-541 339.5,-541 339.5,-501 339.5,-501 339.5,-495 345.5,-489 351.5,-489 351.5,-489 459,-489 459,-489 465,-489 471,-495 471,-501 471,-501 471,-541 471,-541 471,-547 465,-553 459,-553\"/>\n",
       "<text text-anchor=\"start\" x=\"377.88\" y=\"-536.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">x</text>\n",
       "<text text-anchor=\"start\" x=\"384.62\" y=\"-536.7\" font-family=\"Helvetica,sans-Serif\" baseline-shift=\"sub\" font-size=\"14.00\">2</text>\n",
       "<text text-anchor=\"start\" x=\"391.38\" y=\"-536.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"> ≤ 2.45</text>\n",
       "<text text-anchor=\"start\" x=\"370\" y=\"-522.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.664</text>\n",
       "<text text-anchor=\"start\" x=\"360.62\" y=\"-508.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 105</text>\n",
       "<text text-anchor=\"start\" x=\"347.5\" y=\"-494.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [31, 37, 37]</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M382.5,-447.5C382.5,-447.5 290,-447.5 290,-447.5 284,-447.5 278,-441.5 278,-435.5 278,-435.5 278,-406.5 278,-406.5 278,-400.5 284,-394.5 290,-394.5 290,-394.5 382.5,-394.5 382.5,-394.5 388.5,-394.5 394.5,-400.5 394.5,-406.5 394.5,-406.5 394.5,-435.5 394.5,-435.5 394.5,-441.5 388.5,-447.5 382.5,-447.5\"/>\n",
       "<text text-anchor=\"start\" x=\"308.5\" y=\"-430.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"295.38\" y=\"-415.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 31</text>\n",
       "<text text-anchor=\"start\" x=\"286\" y=\"-400.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [31, 0, 0]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M383.23,-488.72C376.1,-478.6 368.15,-467.31 360.85,-456.94\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"363.86,-455.14 355.24,-448.98 358.14,-459.17 363.86,-455.14\"/>\n",
       "<text text-anchor=\"middle\" x=\"350.09\" y=\"-466.57\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M524.25,-453C524.25,-453 424.25,-453 424.25,-453 418.25,-453 412.25,-447 412.25,-441 412.25,-441 412.25,-401 412.25,-401 412.25,-395 418.25,-389 424.25,-389 424.25,-389 524.25,-389 524.25,-389 530.25,-389 536.25,-395 536.25,-401 536.25,-401 536.25,-441 536.25,-441 536.25,-447 530.25,-453 524.25,-453\"/>\n",
       "<text text-anchor=\"start\" x=\"446.88\" y=\"-436.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">x</text>\n",
       "<text text-anchor=\"start\" x=\"453.62\" y=\"-436.7\" font-family=\"Helvetica,sans-Serif\" baseline-shift=\"sub\" font-size=\"14.00\">3</text>\n",
       "<text text-anchor=\"start\" x=\"460.38\" y=\"-436.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"> ≤ 1.75</text>\n",
       "<text text-anchor=\"start\" x=\"446.5\" y=\"-422.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"433.38\" y=\"-408.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 74</text>\n",
       "<text text-anchor=\"start\" x=\"420.25\" y=\"-394.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 37, 37]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M427.27,-488.72C433.12,-480.41 439.53,-471.32 445.67,-462.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"448.43,-464.75 451.33,-454.55 442.71,-460.72 448.43,-464.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"456.48\" y=\"-472.15\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<path fill=\"#54e992\" stroke=\"black\" d=\"M423.5,-353C423.5,-353 331,-353 331,-353 325,-353 319,-347 319,-341 319,-341 319,-301 319,-301 319,-295 325,-289 331,-289 331,-289 423.5,-289 423.5,-289 429.5,-289 435.5,-295 435.5,-301 435.5,-301 435.5,-341 435.5,-341 435.5,-347 429.5,-353 423.5,-353\"/>\n",
       "<text text-anchor=\"start\" x=\"349.88\" y=\"-336.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">x</text>\n",
       "<text text-anchor=\"start\" x=\"356.62\" y=\"-336.7\" font-family=\"Helvetica,sans-Serif\" baseline-shift=\"sub\" font-size=\"14.00\">2</text>\n",
       "<text text-anchor=\"start\" x=\"363.38\" y=\"-336.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"> ≤ 4.95</text>\n",
       "<text text-anchor=\"start\" x=\"342\" y=\"-322.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.214</text>\n",
       "<text text-anchor=\"start\" x=\"336.38\" y=\"-308.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 41</text>\n",
       "<text text-anchor=\"start\" x=\"327\" y=\"-294.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 36, 5]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M443.29,-388.72C434.72,-380.06 425.3,-370.54 416.32,-361.48\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"418.83,-359.04 409.31,-354.39 413.86,-363.96 418.83,-359.04\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<path fill=\"#853fe6\" stroke=\"black\" d=\"M616.5,-353C616.5,-353 524,-353 524,-353 518,-353 512,-347 512,-341 512,-341 512,-301 512,-301 512,-295 518,-289 524,-289 524,-289 616.5,-289 616.5,-289 622.5,-289 628.5,-295 628.5,-301 628.5,-301 628.5,-341 628.5,-341 628.5,-347 622.5,-353 616.5,-353\"/>\n",
       "<text text-anchor=\"start\" x=\"542.88\" y=\"-336.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">x</text>\n",
       "<text text-anchor=\"start\" x=\"549.62\" y=\"-336.7\" font-family=\"Helvetica,sans-Serif\" baseline-shift=\"sub\" font-size=\"14.00\">2</text>\n",
       "<text text-anchor=\"start\" x=\"556.38\" y=\"-336.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"> ≤ 4.85</text>\n",
       "<text text-anchor=\"start\" x=\"535\" y=\"-322.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.059</text>\n",
       "<text text-anchor=\"start\" x=\"529.38\" y=\"-308.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 33</text>\n",
       "<text text-anchor=\"start\" x=\"520\" y=\"-294.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 32]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>2&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M504.89,-388.72C513.38,-380.06 522.7,-370.54 531.58,-361.48\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"534.02,-363.99 538.51,-354.4 529.02,-359.09 534.02,-363.99\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<path fill=\"#3fe685\" stroke=\"black\" d=\"M235.5,-253C235.5,-253 143,-253 143,-253 137,-253 131,-247 131,-241 131,-241 131,-201 131,-201 131,-195 137,-189 143,-189 143,-189 235.5,-189 235.5,-189 241.5,-189 247.5,-195 247.5,-201 247.5,-201 247.5,-241 247.5,-241 247.5,-247 241.5,-253 235.5,-253\"/>\n",
       "<text text-anchor=\"start\" x=\"165.62\" y=\"-236.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">x</text>\n",
       "<text text-anchor=\"start\" x=\"172.38\" y=\"-236.7\" font-family=\"Helvetica,sans-Serif\" baseline-shift=\"sub\" font-size=\"14.00\">3</text>\n",
       "<text text-anchor=\"start\" x=\"179.12\" y=\"-236.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"> ≤ 1.6</text>\n",
       "<text text-anchor=\"start\" x=\"154\" y=\"-222.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.056</text>\n",
       "<text text-anchor=\"start\" x=\"148.38\" y=\"-208.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 35</text>\n",
       "<text text-anchor=\"start\" x=\"139\" y=\"-194.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 34, 1]</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M318.54,-289.4C299.28,-279.35 277.7,-268.11 257.78,-257.72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"259.67,-254.76 249.18,-253.24 256.43,-260.97 259.67,-254.76\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<path fill=\"#c09cf2\" stroke=\"black\" d=\"M419.75,-253C419.75,-253 334.75,-253 334.75,-253 328.75,-253 322.75,-247 322.75,-241 322.75,-241 322.75,-201 322.75,-201 322.75,-195 328.75,-189 334.75,-189 334.75,-189 419.75,-189 419.75,-189 425.75,-189 431.75,-195 431.75,-201 431.75,-201 431.75,-241 431.75,-241 431.75,-247 425.75,-253 419.75,-253\"/>\n",
       "<text text-anchor=\"start\" x=\"349.88\" y=\"-236.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">x</text>\n",
       "<text text-anchor=\"start\" x=\"356.62\" y=\"-236.7\" font-family=\"Helvetica,sans-Serif\" baseline-shift=\"sub\" font-size=\"14.00\">3</text>\n",
       "<text text-anchor=\"start\" x=\"363.38\" y=\"-236.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"> ≤ 1.55</text>\n",
       "<text text-anchor=\"start\" x=\"342\" y=\"-222.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n",
       "<text text-anchor=\"start\" x=\"340.12\" y=\"-208.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n",
       "<text text-anchor=\"start\" x=\"330.75\" y=\"-194.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 4]</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>3&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M377.25,-288.72C377.25,-281.12 377.25,-272.86 377.25,-264.82\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"380.75,-264.83 377.25,-254.83 373.75,-264.83 380.75,-264.83\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<path fill=\"#39e581\" stroke=\"black\" d=\"M104.5,-147.5C104.5,-147.5 12,-147.5 12,-147.5 6,-147.5 0,-141.5 0,-135.5 0,-135.5 0,-106.5 0,-106.5 0,-100.5 6,-94.5 12,-94.5 12,-94.5 104.5,-94.5 104.5,-94.5 110.5,-94.5 116.5,-100.5 116.5,-106.5 116.5,-106.5 116.5,-135.5 116.5,-135.5 116.5,-141.5 110.5,-147.5 104.5,-147.5\"/>\n",
       "<text text-anchor=\"start\" x=\"30.5\" y=\"-130.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"17.38\" y=\"-115.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 34</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-100.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 34, 0]</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M147.44,-188.72C132.81,-177.78 116.35,-165.46 101.59,-154.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"103.97,-151.83 93.87,-148.64 99.78,-157.44 103.97,-151.83\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<path fill=\"#8139e5\" stroke=\"black\" d=\"M231.75,-147.5C231.75,-147.5 146.75,-147.5 146.75,-147.5 140.75,-147.5 134.75,-141.5 134.75,-135.5 134.75,-135.5 134.75,-106.5 134.75,-106.5 134.75,-100.5 140.75,-94.5 146.75,-94.5 146.75,-94.5 231.75,-94.5 231.75,-94.5 237.75,-94.5 243.75,-100.5 243.75,-106.5 243.75,-106.5 243.75,-135.5 243.75,-135.5 243.75,-141.5 237.75,-147.5 231.75,-147.5\"/>\n",
       "<text text-anchor=\"start\" x=\"161.5\" y=\"-130.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"152.12\" y=\"-115.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n",
       "<text text-anchor=\"start\" x=\"142.75\" y=\"-100.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>4&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M189.25,-188.72C189.25,-179.32 189.25,-168.92 189.25,-159.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"192.75,-159.25 189.25,-149.25 185.75,-159.25 192.75,-159.25\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<path fill=\"#8139e5\" stroke=\"black\" d=\"M358.75,-147.5C358.75,-147.5 273.75,-147.5 273.75,-147.5 267.75,-147.5 261.75,-141.5 261.75,-135.5 261.75,-135.5 261.75,-106.5 261.75,-106.5 261.75,-100.5 267.75,-94.5 273.75,-94.5 273.75,-94.5 358.75,-94.5 358.75,-94.5 364.75,-94.5 370.75,-100.5 370.75,-106.5 370.75,-106.5 370.75,-135.5 370.75,-135.5 370.75,-141.5 364.75,-147.5 358.75,-147.5\"/>\n",
       "<text text-anchor=\"start\" x=\"288.5\" y=\"-130.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"279.12\" y=\"-115.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n",
       "<text text-anchor=\"start\" x=\"269.75\" y=\"-100.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 3]</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M357.78,-188.72C351.55,-178.7 344.6,-167.54 338.2,-157.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"341.33,-155.66 333.07,-149.02 335.38,-159.36 341.33,-155.66\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<path fill=\"#9cf2c0\" stroke=\"black\" d=\"M485.75,-153C485.75,-153 400.75,-153 400.75,-153 394.75,-153 388.75,-147 388.75,-141 388.75,-141 388.75,-101 388.75,-101 388.75,-95 394.75,-89 400.75,-89 400.75,-89 485.75,-89 485.75,-89 491.75,-89 497.75,-95 497.75,-101 497.75,-101 497.75,-141 497.75,-141 497.75,-147 491.75,-153 485.75,-153\"/>\n",
       "<text text-anchor=\"start\" x=\"415.88\" y=\"-136.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">x</text>\n",
       "<text text-anchor=\"start\" x=\"422.62\" y=\"-136.7\" font-family=\"Helvetica,sans-Serif\" baseline-shift=\"sub\" font-size=\"14.00\">0</text>\n",
       "<text text-anchor=\"start\" x=\"429.38\" y=\"-136.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"> ≤ 6.95</text>\n",
       "<text text-anchor=\"start\" x=\"408\" y=\"-122.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n",
       "<text text-anchor=\"start\" x=\"406.12\" y=\"-108.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n",
       "<text text-anchor=\"start\" x=\"396.75\" y=\"-94.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 1]</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>7&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M398.31,-188.72C403.91,-180.41 410.04,-171.32 415.91,-162.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"418.63,-164.82 421.31,-154.57 412.82,-160.91 418.63,-164.82\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<path fill=\"#39e581\" stroke=\"black\" d=\"M421.75,-53C421.75,-53 336.75,-53 336.75,-53 330.75,-53 324.75,-47 324.75,-41 324.75,-41 324.75,-12 324.75,-12 324.75,-6 330.75,0 336.75,0 336.75,0 421.75,0 421.75,0 427.75,0 433.75,-6 433.75,-12 433.75,-12 433.75,-41 433.75,-41 433.75,-47 427.75,-53 421.75,-53\"/>\n",
       "<text text-anchor=\"start\" x=\"351.5\" y=\"-35.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"342.12\" y=\"-20.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n",
       "<text text-anchor=\"start\" x=\"332.75\" y=\"-5.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 0]</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M421.58,-88.68C415.79,-80.31 409.48,-71.2 403.55,-62.63\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"406.5,-60.73 397.93,-54.5 400.74,-64.71 406.5,-60.73\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<path fill=\"#8139e5\" stroke=\"black\" d=\"M548.75,-53C548.75,-53 463.75,-53 463.75,-53 457.75,-53 451.75,-47 451.75,-41 451.75,-41 451.75,-12 451.75,-12 451.75,-6 457.75,0 463.75,0 463.75,0 548.75,0 548.75,0 554.75,0 560.75,-6 560.75,-12 560.75,-12 560.75,-41 560.75,-41 560.75,-47 554.75,-53 548.75,-53\"/>\n",
       "<text text-anchor=\"start\" x=\"478.5\" y=\"-35.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"469.12\" y=\"-20.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n",
       "<text text-anchor=\"start\" x=\"459.75\" y=\"-5.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>9&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M464.58,-88.68C470.28,-80.31 476.49,-71.2 482.33,-62.63\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"485.12,-64.74 487.86,-54.5 479.34,-60.8 485.12,-64.74\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<path fill=\"#c09cf2\" stroke=\"black\" d=\"M612.75,-253C612.75,-253 527.75,-253 527.75,-253 521.75,-253 515.75,-247 515.75,-241 515.75,-241 515.75,-201 515.75,-201 515.75,-195 521.75,-189 527.75,-189 527.75,-189 612.75,-189 612.75,-189 618.75,-189 624.75,-195 624.75,-201 624.75,-201 624.75,-241 624.75,-241 624.75,-247 618.75,-253 612.75,-253\"/>\n",
       "<text text-anchor=\"start\" x=\"542.88\" y=\"-236.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">x</text>\n",
       "<text text-anchor=\"start\" x=\"549.62\" y=\"-236.7\" font-family=\"Helvetica,sans-Serif\" baseline-shift=\"sub\" font-size=\"14.00\">0</text>\n",
       "<text text-anchor=\"start\" x=\"556.38\" y=\"-236.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\"> ≤ 5.95</text>\n",
       "<text text-anchor=\"start\" x=\"535\" y=\"-222.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n",
       "<text text-anchor=\"start\" x=\"533.12\" y=\"-208.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n",
       "<text text-anchor=\"start\" x=\"523.75\" y=\"-194.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 2]</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M570.25,-288.72C570.25,-281.12 570.25,-272.86 570.25,-264.82\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"573.75,-264.83 570.25,-254.83 566.75,-264.83 573.75,-264.83\"/>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>16</title>\n",
       "<path fill=\"#8139e5\" stroke=\"black\" d=\"M747.5,-247.5C747.5,-247.5 655,-247.5 655,-247.5 649,-247.5 643,-241.5 643,-235.5 643,-235.5 643,-206.5 643,-206.5 643,-200.5 649,-194.5 655,-194.5 655,-194.5 747.5,-194.5 747.5,-194.5 753.5,-194.5 759.5,-200.5 759.5,-206.5 759.5,-206.5 759.5,-235.5 759.5,-235.5 759.5,-241.5 753.5,-247.5 747.5,-247.5\"/>\n",
       "<text text-anchor=\"start\" x=\"673.5\" y=\"-230.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"660.38\" y=\"-215.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 30</text>\n",
       "<text text-anchor=\"start\" x=\"651\" y=\"-200.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 30]</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;16 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>12&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M612.06,-288.72C626.69,-277.78 643.15,-265.46 657.91,-254.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"659.72,-257.44 665.63,-248.64 655.53,-251.83 659.72,-257.44\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>14</title>\n",
       "<path fill=\"#39e581\" stroke=\"black\" d=\"M612.75,-147.5C612.75,-147.5 527.75,-147.5 527.75,-147.5 521.75,-147.5 515.75,-141.5 515.75,-135.5 515.75,-135.5 515.75,-106.5 515.75,-106.5 515.75,-100.5 521.75,-94.5 527.75,-94.5 527.75,-94.5 612.75,-94.5 612.75,-94.5 618.75,-94.5 624.75,-100.5 624.75,-106.5 624.75,-106.5 624.75,-135.5 624.75,-135.5 624.75,-141.5 618.75,-147.5 612.75,-147.5\"/>\n",
       "<text text-anchor=\"start\" x=\"542.5\" y=\"-130.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"533.12\" y=\"-115.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n",
       "<text text-anchor=\"start\" x=\"523.75\" y=\"-100.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 0]</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>13&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M570.25,-188.72C570.25,-179.32 570.25,-168.92 570.25,-159.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"573.75,-159.25 570.25,-149.25 566.75,-159.25 573.75,-159.25\"/>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>15</title>\n",
       "<path fill=\"#8139e5\" stroke=\"black\" d=\"M739.75,-147.5C739.75,-147.5 654.75,-147.5 654.75,-147.5 648.75,-147.5 642.75,-141.5 642.75,-135.5 642.75,-135.5 642.75,-106.5 642.75,-106.5 642.75,-100.5 648.75,-94.5 654.75,-94.5 654.75,-94.5 739.75,-94.5 739.75,-94.5 745.75,-94.5 751.75,-100.5 751.75,-106.5 751.75,-106.5 751.75,-135.5 751.75,-135.5 751.75,-141.5 745.75,-147.5 739.75,-147.5\"/>\n",
       "<text text-anchor=\"start\" x=\"669.5\" y=\"-130.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"660.12\" y=\"-115.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n",
       "<text text-anchor=\"start\" x=\"650.75\" y=\"-100.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 2]</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;15 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>13&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M610.78,-188.72C624.84,-177.88 640.62,-165.7 654.83,-154.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"656.92,-157.54 662.7,-148.66 652.64,-152 656.92,-157.54\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x11d58f990>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "import graphviz\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load iris dataset\n",
    "data = load_iris()\n",
    "\n",
    "# separate features from class labels\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# data splitting (training and test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion='gini')\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# use model for predicting test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# choose metric and evaluate the model's performance\n",
    "print(\"Decision Tree Classification accuracy:\", metrics.accuracy_score(y_test, y_pred)*100)\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None,\n",
    "    filled=True, rounded=True,special_characters=True)\n",
    "\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec8c680-42da-429d-b484-d77673ee5d64",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4>Ein paar Anmerkungen zum Overfitting</h4>\n",
    "Ein generelles Problem bei allen Klassifikationsverfahren besteht im so genannten Overfitting. Dieses ist in der folgenden Abbildung exemplarisch für den Entscheidungsbaum dargestellt:<br><br>\n",
    "<p style=\"text-align: center\"><img src=\"pics/OverfittingTree.png\" alt=\"Bild\"><br></p><center><a href=\"https://alanjeffares.wordpress.com/tutorials/decision-tree/\">Quelle</a></center><br>\n",
    "Overfitting bedeutet, dass sich das Lernverfahren zu eng an die Trainingsdaten angepasst hat und dadurch seine Fähigkeit zur Generalisierung verliert. Damit ist die Klassifikations-Qualität der Ergebnisse bei neuen, nicht für das Training verwendeten, Daten gemeint.<br> Bei Entscheidungsbäumen können wir hierauf Einfluss nehmen, indem wir bspw. die maximale Tiefe des Baumes oder die minimale Anzahl an Beispielen pro Knoten vorgeben. Beim Erreichen des Maximums bzw. Minimums wird das Lernverfahren dann abgebrochen. Wir können des Weiteren auch eine Mindestverbesserung hinsichtlich der Reinheit pro Aufteilung vorschreiben. Wird diese nicht erreicht, so wird der entsprechende Knoten automatisch zum Blatt.<br><br>\n",
    "<p style=\"text-align: center\"><img src=\"https://alanjeffares.files.wordpress.com/2018/08/secondgif.gif?w=978&zoom=2\" alt=\"Bild\"><br></p><center><a href=\"https://alanjeffares.wordpress.com/tutorials/decision-tree/\">Quelle</a></center><br><br>\n",
    "Eine weitere Möglichkeit besteht im so genannten <b>Pruning</b> (Zurückschneiden). Hierbei werden bspw. durch Verwendung weiterer (Test-)Daten die Aufteilungen (Splits) identifiziert, die zu einer schlechten Generalisierung führen. Diese werden anschließend entfernt.<br><br> \n",
    "<p style=\"text-align: center\"><img src=\"pics/Pruning.png\" alt=\"Bild\"><br></p><center><a href=\"https://alanjeffares.wordpress.com/tutorials/decision-tree/\">Quelle</a></center><br><br>\n",
    "<p style=\"text-align: center\"><img src=\"pics/Gluehbirne.jpg\" alt=\"Bild\" style=\"height: 220px; width: 300px;\"><br></p><br>\n",
    "    <p style=\"text-align: center\"><b>Overfitting</b> ist ein generelles Problem, dass bei allen Klassifikatoren auftreten kann. Es muss stets beim Training berücksichtigt werden !!!</p><br>\n",
    "<h4>Ensemble Learning - Ein erstes Beispiel</h4>\n",
    "Unter <b>Ensemble Learning</b> verstehen wir die Verwendung und Kombination mehrerer (endlich vieler) Lernverfahren. Angewendet auf Entscheidungsbäume kann bspw. folgendermaßen vorgegangen werden:<br>\n",
    "Wir führen ein so genanntes <b>Bagging</b> aus. Das heißt, wir wählen aus den Trainingsdaten $n$ Beispiele zufällig mit Zurücklegen aus. Dies führen wir $m$-mal durch. Auf diesem Wege entstehen $m$ neue Trainingsdatensätze. Dieses Verfahren nennt man <b>Bootstrap aggregating</b> oder auch kurz <b>Bagging</b>. Nun können wir für jeden dieser neuen Trainingsdatensätze einen Entscheidungsbaum trainieren. Bei der Prädiktion, also der Klassifikation eines neuen Beispiels, erhalten wir somit $m$ Ergebnisse. Das Klassenlabel, dass diesem Beispiel zugewiesen wird ist dann bspw. das Klassenlabel, welches am häufigsten vertreten ist. Der so neu entstandene Klassifikator, der also auf einer Anzahl von einzelnen Entscheidungs<b>bäumen</b> basiert, wird <b>Random Forest</b> genannt.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beec4ce6-d9fb-4617-bddb-20b9c1447a6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style=\"text-align: center\"><img src=\"pics/Exercise.png\" alt=\"Bild\" style=\"height: 220px; width: 300px;\"><br></p><br>\n",
    "<b><u>Es ist wieder Zeit für ein bisschen Übung ...</b></u><P>\n",
    "    Verwenden Sie nun nochmals den Iris-Datensatz sowie sklearn um einen Random Forest zu erstellen. Vergleichen Sie die Ergebnisse mit denen des einzelnen Entscheidungsbaumes. Denken Sie auch hier wieder daran, die Daten zuvor wieder in Trainings-, Validierungs- und Testdaten aufzuteilen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc55a8f-09c2-4b50-a737-e1f95bd535d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4>Regressionsbäume</h4>\n",
    "Das bisher geschilderte Vorgehen bezog sich auf Entscheidungsbäume (Decision Trees), die ausschließlich zur Klassifikation verwendet werden. Ein analoges Vorgehen kann jedoch dazu verwendet werden, Bäume auch für die Regression zu nutzen. Wir sprechen dann von Regressionsbäumen (Regression Trees). Wir müssen hierzu lediglich das Vorgehen bei der Aufteilung modfizieren. Hier haben wir bisher entweder den Information Gain oder die Gini Impurity verwendet. Bei Regressionsbäumen hingegen wird die \"optimale\" Aufteilung an einem Knoten dadurch ermittelt, dass der folgende Ausdruck minimiert wird:<br><br>\n",
    "$$SSE = \\sum\\limits_{l\\in leaves}\\sum\\limits_{i \\in l}(y_i-\\bar{y}_l)^2$$<br><br>\n",
    "Hierbei bezeichnet $\\bar{y}_l$ den Mittelwert des Regressanden pro Teilmenge (nach dem Split). Die optimale Aufteilung (Split) an einem Knoten wird also die Daten so in Mengen zusammefassen, dass die Varianz der abhängigen Variablen (Regressand) innerhalb jeder Menge minimal wird. Die Reinheit (Impurity) bezieht sich hier also auf den Wert des Regressanden. In der Anwendung, also der Prädiktion, wird dann einfach der Mittelwert aller Regressanden im finalen Blatt zurückgegeben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3c23d6-1699-4583-81fa-1826a08e1e2d",
   "metadata": {},
   "source": [
    "<a id=\"s5\"></a><h3 style=\"color:rgb(127,203,223)\">3.4 <b>Ada</b>ptive <b>Boost</b>ing</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d93f561-dfcd-445e-8c90-4cd7db78c422",
   "metadata": {},
   "source": [
    "Adaptive Boosting, kurz Adaboost, gehört zu den so genannten Boosting-Verfahren. Diese wiederum können als besondere Formen des Ensemble Learning verstanden werden. Boosting (im Sinne von Verstärken) kombiniert mehrere \"schwache\" Klassifikatoren zu einem stärkeren Klassifikator. Die Idee des Boostings geht auf Robert Schapire zurück, der zusammen mit Yoav Freund 1997 auch die spezielle Variante des Adaboost vorgestellt hat. Adaboost stellt in seiner ursprünglichen From einen binären Klassifikator dar. Zunächst soll die Grundidee anhand eines <b>Toy Examples</b> vorgestellt werden. Im Anschluss daran werden wir die Details des Verfahrens betrachen und auch beweisen.<br><br>\n",
    "Nehmen wir an, dass wir eine Reihe von \"schwachen\" Klassifikatoren (weak learners) sowie eine Menge von Merkmalsvektoren, die jeweils mit einem von zwei möglichen Klassenlabeln attributiert sind, vorliegen haben. Die Merkmalsvektoren sind in dem folgenden Beispiel, dass einer <b>Präsentation von Yoav Freund und Robert Schapire</b> entnommen worden ist, als Punkte in einem Quadrat dargestellt. Die Klassenzugehörigkeit ist durch die Farbe (blau und rot) sowie die Form (Plus- oder Minuszeichen) kodiert. Für jede Klasse liegen 5 Beispiele vor. Jedes Datum bekommt ein Gewicht und wir beginnen mit einem einheitlichen Gewicht für jedes Datum.<br><br>\n",
    "\n",
    "<p style=\"text-align: center\"><img src=\"pics/Adaboost01.png\" alt=\"Bild\" style=\"height: 220px; width: 300px;\"><br></p><br><br>\n",
    "\n",
    "Eine schwache Klassifikation besteht darin, dass das Quadrat mittels einer horizontalen oder vertikalen Linie geteilt wird. Die erste Teilung findet so statt, dass der resultierende Fehler minimal wird. Dies ist in der folgenden Abbildung dargestellt:<br><br>\n",
    "\n",
    "<p style=\"text-align: center\"><img src=\"pics/Adaboost02.png\" alt=\"Bild\" style=\"height: 220px; width: 300px;\"><br></p><br><br>\n",
    "Wir sehen, dass insgesamt drei Elemente (eingekreist dargestellt) falsch klassifiziert worden sind. Überzeugen Sie sich davon, dass sie die beiden Klassen nicht durch das Einzeichnen einer einzigen Linie exakt voneinander trennen können. Wir sagen auch, die Daten sind nicht linear separabel.<br>\n",
    "Wir vernachlässigen zunächst einmal die Zahlen, die neben der Abbildung dargestellt sind, dazu später mehr. Die Gewichte der Beispiele, die falsch klassifiziert worden sind, werden nun erhöht, wohingegen die Gewichte der Beispiele die richtig klassifiziert worden sind verringert werden. Die Größe der Gewichte wird im folgenden durch die Größe der Symbole verdeutlicht.<br><br>\n",
    "\n",
    "<p style=\"text-align: center\"><img src=\"pics/Adaboost03.png\" alt=\"Bild\" style=\"height: 220px; width: 300px;\"><br></p><br><br>\n",
    "\n",
    "Nun suchen wir erneut nach einer horizontalen oder vertikalen Teilung, die den Fehler minimiert. Hierbei ist zu beachten, dass bei der Berechnung der Fehler das Gewicht jedes einzelnen Beispieles berücksichtigt wird. Ein falsch klassifziertes Beispiel mit hohem Gewicht - das Beispiel wurde also bereits zuvor falsch klassifiziert - geht also stärker in den Gesamtfehler als ein falsch klassifiziertes Beispiel mit geringerem Gewicht.<br><br>\n",
    "\n",
    "<p style=\"text-align: center\"><img src=\"pics/Adaboost05.png\" alt=\"Bild\" style=\"height: 220px; width: 300px;\"><br></p><br><br>\n",
    "\n",
    "Auch hier treten wieder drei Fehler auf. Diesmal sind es Beispiele, die zur negativen Klasse gehören. Es werden anschließend wiederum die Gewichte der einzelnen Beispiele angepasst. Die Gewichte der drei falsch klassifizierten negativen Beispiele werden erhöht, alle anderen verringert.<br><br>\n",
    "\n",
    "<p style=\"text-align: center\"><img src=\"pics/Adaboost06.png\" alt=\"Bild\" style=\"height: 220px; width: 300px;\"><br></p><br><br>\n",
    "\n",
    "Kommen wir nun zur dritten und in diesem Beispiel letzten Runde. Es findet erneut eine Teilung statt, die wie folgt aussieht<br><br>\n",
    "\n",
    "<p style=\"text-align: center\"><img src=\"pics/Adaboost09.png\" alt=\"Bild\" style=\"height: 220px; width: 300px;\"><br></p><br><br>\n",
    "\n",
    "Nun können wir die drei einzelnen Klassifikatoren (Teilungen) zu einer gewichteten Summe, hier kommen die $\\alpha$'s zum Einsatz, zusammenfassen. Jede einzelne Klassifikation liefert einen Wert aus der Menge $\\left\\{-1,1\\right\\}$, je nachdem ob das entsprechende Beispiel der positiven oder der negativen Klasse zugeordnet wird. Die Signum-Funktion, die auf der gewichteten Summe angewendet wird, liefert nun entweder ein positives oder negatives Vorzeichen und votiert damit endgültig für eine der beiden möglichen Klassen. Überzeugen Sie sich, dass mit diesem Klassifikator alle Trainingsbeispiele richtig klassifiziert werden. <br><br>\n",
    "\n",
    "<p style=\"text-align: center\"><img src=\"pics/Adaboost10.png\" alt=\"Bild\" style=\"height: 220px; width: 950px;\"><br></p><br><br>\n",
    "\n",
    "<p style=\"text-align: center\"><img src=\"pics/AdaboostFinal.png\" alt=\"Bild\" style=\"height: 220px; width: 220px;\"><br></p><br><br>\n",
    "\n",
    "Schauen wir uns nun an, wie dieses Verfahren im Detail funktioniert, was die in jedem Durchlauf berechneten $\\epsilon$ und $\\alpha$ bedeuten und wie sie berechnet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b963fc2c-9831-43a3-807b-26d2cb662310",
   "metadata": {
    "tags": []
   },
   "source": [
    "Zunächst wollen wir den formalen Algorithmus angeben und dann streng mathematisch beweisen, dass der Fehler des resultierenden Klassifikators mit zunehmender Anzahl an Runden gegen Null konvergiert, so der Fehler eines <b>jeden</b> schwachen Klassifikators nur <b>kleiner 50%</b> ausfällt.<br>\n",
    "\n",
    "<h5><u>Adaboost-Algorithmus</u></h5><br>\n",
    "<ul>\n",
    "    <li> Gegeben sei die Trainingsmenge $X=\\left\\{(\\underline{x}_1,y_1),\\dots,(\\underline{x}_m,y_m\\right\\}$ wobei $y_i \\in \\{-1,+1\\}$ die Klassenlabel darstellen (Ground Truth).</li>\n",
    "    <li> for $t=1,\\dots,T$ do\n",
    "        <ul>\n",
    "            <li> konstruiere die Gewichtsverteilung $D_t$ der Trainingsdaten $\\{1,\\dots,m\\}$</li>\n",
    "            <li> Finde den schwachen Klassifikator $h_t:X \\longrightarrow \\{-1,+1\\}$ mit dem kleinsten Fehler $\\epsilon_t = P_{D_t}[h_t(\\underline{x}_i) \\neq y_i]=\\sum\\limits_{\\substack{i=1 \\\\ h_t(\\underline{x}_i) \\neq y_i}}^m D_t(i)$</li>\n",
    "        </ul>\n",
    "    <li>Erstelle die finale Hypothese $H_{final}(\\underline{x}):=sign\\left(\\sum\\limits_{t=1}^T \\alpha_t\\,h_t(\\underline{x})\\right)$</li>   \n",
    "</ul>     \n",
    "Wie konstruieren wir nun die Gewichtsverteilung?<br>\n",
    "Wir beginnen damit, dass wir zu Beginn jedem Trainings-Beispiel dasselbe Gewicht zuweisen. Gewichte sollen jeweils nichnegative Zahlen darstellen, die sich zu Eins addieren. In den folgenden Schritten erhöhen wir das Gewicht der Beispiele, die falsch klassifiziert worden sind und erniedrigen die, die richtig klassifiziert worden sind:<br>\n",
    "<ul>\n",
    "    <li>$D_i(i):=\\frac{1}{m}$</li>\n",
    "    <li>Sind $D_t$ und $h_t$ gegeben, so berechnen sich die Gewichte für den nächsten Schritt zu:\n",
    "        $$D_{t+1}(i):=\\frac{D_t(i)}{Z_t}\\cdot \\left\\{ \\begin{array}{ll} e^{-\\alpha_t} & falls & y_i=h_t(\\underline{x}_i) \\\\ e^{\\alpha_t} & falls & y_i \\neq h_t(\\underline{x}_i) \\\\ \\end{array}\\right .$$\n",
    "    Hierbei stellt $Z_t$ eine Normalisierungskonstante dar, die sicherstellt, dass die Summe aller Gewichte wieder Eins ergibt. Der Wert von $\\alpha_t$ brechnet sich wie folgt:\n",
    "     $$\\alpha_t := \\frac{1}{2}\\,ln \\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right)>0$$</li>\n",
    "</ul>\n",
    "<h5><u>Konvergenzbeweis</u></h5><br>\n",
    "Wir wollen nun beweisen, dass der Fehler mit zunehmender Anzahl an Runden gegen Null konvergiert solange nur jeder beste schwache Klassifikator einen Fehler von kleiner 50% aufweist:<br><br> \n",
    "Nehmen wir an, dass in jeder Runde $t$ gilt: $\\epsilon_t=\\frac{1}{2}-\\gamma_t$ mit $\\gamma_t \\geq \\gamma >0$ für alle $t$<br>\n",
    "Wir definieren $f(\\underline{x}):=\\sum\\limits_{t=1}^T \\alpha_t\\,h_t(\\underline{x})$, dann gilt: $H_{final}(\\underline{x})=sign\\left(f(\\underline{x})\\right)$<br>\n",
    "Die finale Gewichtsverteilung berechnet sich zu: \n",
    "$$D_T(i) = \\frac{1}{m}\\cdot \\frac{e^{-y_i\\,\\sum\\limits_{t=1}^T \\alpha_t\\,h_t(\\underline{x}_i)}}{\\prod\\limits_{t=1}^T Z_t}=\\frac{1}{m}\\cdot \\frac{e^{-y_i\\,f(\\underline{x}_i)}}{\\prod\\limits_{t=1}^T Z_t}$$\n",
    "Der Fehler des finalen Klassifikators auf der Trainingsmenge berechnet sich zu:<br><br>\n",
    "$$\\mbox{Trainingsfehler}(H_{final})=\\frac{1}{m}\\cdot \\sum\\limits_{i=1}^m \\left\\{\\begin{array}{ll} 1 & falls & y_i \\neq H_{final}(\\underline{x}_i) \\\\ 0 & sonst & \\\\ \\end{array} \\right. \\leq \\frac{1}{m} \\cdot \\sum\\limits_{i=1}^m e^{-y_i\\,f(\\underline{x}_i)}=\\sum\\limits_{i=1}^m D_{final}(i)\\,\\prod\\limits_{t=1}^T Z_t = \\prod\\limits_{t=1}^T Z_t$$\n",
    "Wir können nun $Z_t$ durch den jeweiligen Fehler ausdrücken:\n",
    "$$\\begin{eqnarray} Z_t &=& \\sum\\limits_{i=1}^m D_t(i)\\,e^{-\\alpha_t \\,y_i\\,h_t(\\underline{x}_i)} \\\\\n",
    "&=& \\sum\\limits_{\\substack{i=1 \\\\ y_i \\neq h_t(\\underline{x}_i)}}^m D_t(i)\\,e^{\\alpha_t} + \\sum\\limits_{\\substack{i=1 \\\\ y_i=h_t(\\underline{x}_i)}}^m D_t(i)\\, e^{-\\alpha_t} \\\\\n",
    "&=& \\epsilon_t \\, e^{\\alpha_t}+(1-\\epsilon_t)\\,e ^{-\\alpha_t} \\\\ &=& 2\\,\\sqrt{\\epsilon_t\\,(1-\\epsilon_t)}\\end{eqnarray}$$<br><br>\n",
    "Nun können wir die Abschätzung des Fehlers des finalen Klassifikators abschließen:<br><br>\n",
    "$$\\begin{eqnarray} \\mbox{Trainingsfehler}(H_{final}) &\\leq& \\prod\\limits_{t=1}^T Z_t \\\\\n",
    "&=& \\prod\\limits_{t=1}^T 2 \\, \\sqrt{\\epsilon_t \\,(1-\\epsilon_t)} \\\\ &=& \\prod\\limits_{t=1}^T \\sqrt{1-4\\,\\gamma_t^2} \\\\ &\\leq& e^{-2\\,\\sum\\limits_{t=1}^T\\gamma_t^2} \\\\ &\\leq& e^{-2\\,\\gamma^2 \\ T} \\end{eqnarray} $$<br><br>\n",
    "Beim vorvorletzten Schritt haben wir Gebrauch gemacht vom Zusammenhang zwischen $\\epsilon_t$ und $\\gamma_t$. Beim vorletzten Schritt hingegen von der Abschätzung $e^x \\geq 1+x$!!!<br>\n",
    "Würden wir die Gesamtzahl an Runden $T$ nun gegen unendlich gehen lassen, so geht der Fehler des finalen Klassifikators gegen Null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1686e2b-fbda-49fe-abd1-1b53bf859d4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df85fc23081e4a6e92657a995dd687c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5000, description='numberOfEstimators', max=10000, min=1), Output()), _d…"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Motivated by example of Noel Dawe (https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-twoclass-py)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from ipywidgets import interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def adaboost(numberOfEstimators):\n",
    "        # create sample data\n",
    "        X1, y1 = make_gaussian_quantiles(mean=(0,0), cov=2.0, n_samples=200, n_features=2, n_classes=2, random_state=42)\n",
    "        X2, y2 = make_gaussian_quantiles(mean=(3,3), cov=1.5, n_samples=300, n_features=2, n_classes=2, random_state=42)\n",
    "\n",
    "        X = np.concatenate((X1,X2))\n",
    "        y = np.concatenate((y1,-y2+1))\n",
    "\n",
    "        clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),algorithm=\"SAMME\",n_estimators=numberOfEstimators, random_state=42)\n",
    "        clf = clf.fit(X, y)\n",
    "\n",
    "        plot_colors = \"br\"\n",
    "        plot_step = 0.02\n",
    "        class_names = \"AB\"\n",
    "\n",
    "        plt.figure(figsize=(20, 10))\n",
    "\n",
    "        # Plot the decision boundaries\n",
    "        ax = plt.subplot(121)\n",
    "        disp = DecisionBoundaryDisplay.from_estimator(clf, X, cmap=plt.cm.Paired, response_method=\"predict\", ax=ax, xlabel=\"x\",ylabel=\"y\")\n",
    "         \n",
    "        x_min, x_max = disp.xx0.min(), disp.xx0.max()\n",
    "        y_min, y_max = disp.xx1.min(), disp.xx1.max()\n",
    "        plt.axis(\"tight\")\n",
    "\n",
    "        # Plot the training points\n",
    "        for i, n, c in zip(range(2), class_names, plot_colors):\n",
    "                idx = np.where(y == i)\n",
    "                plt.scatter(X[idx, 0], X[idx, 1], c=c, cmap=plt.cm.Paired, s=20, edgecolor=\"k\", label=\"Class %s\" % n)\n",
    "        \n",
    "        plt.xlim(x_min, x_max)\n",
    "        plt.ylim(y_min, y_max)\n",
    "        plt.legend(loc=\"upper right\")\n",
    "\n",
    "        plt.title(\"Decision Boundary\")\n",
    "        plt.show()\n",
    "        \n",
    "        # show current performance on training-set only\n",
    "        y_pred = clf.predict(X)\n",
    "        # choose metric and evaluate the model's performance\n",
    "        print(\"Adaboost Classification accuracy on training set:\", metrics.accuracy_score(y, y_pred)*100)\n",
    "        \n",
    "# perform actual regression\n",
    "v=interactive(adaboost,numberOfEstimators=(1,10000))\n",
    "v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abf958f-a949-4f87-945e-0bbae876a06d",
   "metadata": {},
   "source": [
    "Für das Adaboost-Verfahren, dass so wie wir es eingeführt haben, ein binäres Klassifikationsverfahren darstellt, gibt es auch Erweiterungen auf die Multiklassen-Klassifikation. Eine solche Implementierung, die auch in sklearn zum Einsatz kommt, wird in diesem <a href=\"https://hastie.su.domains/Papers/SII-2-3-A8-Zhu.pdf\">Artikel</a> beschrieben.<br><br>\n",
    "<h4>Anwendungsbeispiel von Adaboost im Bereich Computervision</h4>\n",
    "Ein prominentes Beispiel zu Anwendung von Adaboost im Bereich Computervision ist unter dem Begriff <a href=\"https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework\">Viola-Jones-Methode</a> bekannt geworden. Dieses Verfahren wurde 2001 von Paul Viola und Michael Jones im Artikel <a href =\"https://www.google.de/url?sa=t&source=web&rct=j&opi=89978449&url=https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf&ved=2ahUKEwiV_5faiPGFAxVVBtsEHXh0DFwQFnoECAYQAQ&usg=AOvVaw27yCB2tUSGu6jhcPRte6HS\">Rapid Object Detection using a Boosted Cascade of Simple\n",
    "Features</a> vorgestellt. Das Script zum Ausführen (dies muss lokal erfolgen, da das Skript Zugriff auf die lokale webcam benötigt), ist unter dem folgenden <a href=\"code/Viola-Jones/vj.py\">Link</a> zu finden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8de3d4-3cea-4b5a-88fe-733a46d29a3a",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center\"><img src=\"pics/Exercise.png\" alt=\"Bild\" style=\"height: 220px; width: 300px;\"><br></p><br>\n",
    "<b><u>Es ist wieder Zeit für ein bisschen Übung ...</b></u><P>\n",
    "    Verwenden Sie nochmals den Iris-Datensatz sowie sklearn um nun mittels Adaboost einen Klassifikator zu erstellen. Lassen Sie sich neben der Accuracy auch die Confusion-Matrix ausgeben (schauen Sie hierzu in das Paket <b>metrics</b>).<br>\n",
    "Denken Sie auch hier wieder daran, die Daten zuvor wieder in Trainings-, Validierungs- und Testdaten aufzuteilen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c51e5411-c189-47bf-8a16-b43171e0034a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85463da4b7cf46198e23dc632d21af0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=150, description='numberOfEstimators', max=300, min=1), Output()), _dom_…"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ipywidgets import interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "def adaboost(numberOfEstimators):\n",
    "    # load iris dataset\n",
    "    data = load_iris()\n",
    "\n",
    "    # separate features from class labels\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "\n",
    "    # data splitting (training and test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "    clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=numberOfEstimators, random_state=42)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "\n",
    "    # use model for predicting test data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # choose metric and evaluate the model's performance\n",
    "    print(\"AdaBoost Classification accuracy:\", metrics.accuracy_score(y_test, y_pred)*100)\n",
    "    print(\"AdaBoost Confusion Matrix:\\n\\n\", metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# perform actual regression\n",
    "v=interactive(adaboost,numberOfEstimators=(1,300))\n",
    "v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1a87f7-9b41-4394-a6ba-6fc49c2e92bf",
   "metadata": {},
   "source": [
    "<a id=\"s6\"></a><h3 style=\"color:rgb(127,203,223)\">3.5 Bias-Variance-Tradeoff und Cross-Validation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b3b27-10ee-483d-8d4a-fb41fc00c21a",
   "metadata": {},
   "source": [
    "Das <i>Bias Variance Tradeoff</i> oder Verzerrungs-Varianz-Dilemma beschreibt den Konflikt, gleichzeitig zwei Fehlerquellen beim überwachten Lernen zu minimieren. Zum Einen die Verzerrung (bias) und zum anderen die Varianz (variance). Beide haben einen Einfluss auf die Generalisierung eines Modells beim überwachten Lernen. Beide Fehlerquellen sollen anhand der folgenden Abbildung erläutert werden:<br><br>\n",
    "<p style=\"text-align: center\"><img src=\"pics/Varianz_und_Bias.png\" alt=\"Bild\" style=\"height: 400px; width: 400px;\"><br>Quelle: Wikipedia</p><br><br>\n",
    "<p style=\"text-align: center\"><img src=\"pics/Richtigkeit_und_Präzision.png\" alt=\"Bild\" style=\"height: 350px; width: 400px;\"><br>Quelle: Wikipedia</p><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c3439c-5101-41e5-9a02-ee51758fa667",
   "metadata": {},
   "source": [
    "Die Verzerrung (bias) beschreibt einen systematischen Fehler im Lernverfahren. Der Zusammenhang zwischen Input und Output ist nicht korrekt erfasst worden. In diesem Fall würden wir von einer Unteranpassung des Lernverfahrens sprechen, dem so genannten <b>underfitting</b>. Bei einer linearen Regression würde eine höhere Verzerrung beispielsweise dann auftreten, wenn der Grad des Polynoms zu klein gewählt wird.<br>\n",
    "Eine höhere Varianz (variance) hingegen basiert auf einer besonderen Empfindlichkeit des Lernverfahrens auf klein(st)e Veränderungen in den Trainingsdaten. Dies tritt beispielsweise dann auf, wenn das Lernverfahren nicht (nur) den eigentlichen funktionalen Zusammenhang zwischen Input und Output \"erlernt\" hat, sondern darüber hinaus auch das für den konkret verwendeten Trainingsdatensatz charakteristische Rauschen. Hier sprechen wir dann auch vom so genannten <b>overfitting</b> oder Auswendiglernen. Bei der linearen Regression tritt dies beispielsweise dann auf, wenn der Grad des Polynoms zu groß gewählt worden ist.<br>\n",
    "Zu diesen beiden Fehlerquellen kommt noch der so genannte irreduzible Fehler, der aufgrund von Rauschen (in den Trainingsdaten) unweigerlich vorhanden ist. Das Bias-Variance-Tradeoff versucht den erwarteten Generalisierungsfehler eines Lernverfahrens als Summe dieser drei Fehlerquellen zu analysieren. Sie haben bereits an einigen Beispielen sowohl der Klassifikation als auch der Regression erfahren können, dass die Minimierung der Verzerrung als auch der Varianz eine grundsätzliche Herausforderung beim überwachten maschinellen Lernen darstellt und ein möglichst geringer Fehler auf den Trainingsdaten nicht gleichbedeutend ist mit einer guten Generalisierung auf neue, bisher nicht berücksichtigte Daten (Testdaten). Wir wollen die grundlegenden Zusammenhänge noch einmal mathematisch anhand eines Regressionsbeispieles analysieren.Gehen wir hierzu von folgendem Zusammenhang aus:<br>\n",
    "Wir haben eine Trainingsmenge $D:=\\left\\{(x_1,y_1),\\dots,(x_n,y_n)\\right\\}$ sowie den folgenden Zusammenhang zwischen \"Merkmalen\" und einer Größe $y$:<br><br>\n",
    "$$y_i = f(x_i)+\\epsilon \\hspace{1cm} \\mbox{hierbei ist }\\epsilon \\mbox{ eine Zufallsvariable mit }E[\\epsilon]=0 \\mbox{ und } Var[\\epsilon]=\\sigma^2 $$<br>\n",
    "Wir wollen nun die mittlere quadratische Abweichung zwischen y sowie $\\hat{f}(x;D)$ für ein neues Datenpaar $(x,y)$ berechnen. $\\hat{f}$ stellt dabei eine auf der Trainingsmenge $D$ trainierte Regressionsfunktion dar, die an der Stelle $x$ ausgewertet wird. Wir berechnen folglich:<br><br>\n",
    "$$E_{D,\\epsilon}[(y-\\hat{f}(x;D))^2)]$$<br><br>\n",
    "Hierbei erfolgt die Mittlung über alle möglichen Trainingsmengen $D$ sowie alle möglichen Werte der \"Rauschvariablen\" $\\epsilon$. Dieser Ausdruck gestattet es uns die \"Güte\" unseres Klassifikators zu beschreiben. Unter Ausnutzung der folgenden Zusammenhänge:<br><br>\n",
    "$$Var[X] = E[X^2]-(E[X])^2 \\mbox{ sowie }E_{D,\\epsilon}[f(x)]=f(x) \\mbox{ erhalten wir:}$$<br>\n",
    "\\begin{eqnarray}\n",
    "E_{D,\\epsilon}\\left[\\left(y-\\hat{f}(x;D)\\right)^2\\right]&=&E_{D,\\epsilon}[y^2-2\\,y\\,\\hat{f}(x;D)+\\hat{f}(x;D)^2]\\\\\n",
    "&=& E_{\\epsilon}[y^2]+E_{D,\\epsilon}[\\hat{f}(x;D)^2]-2\\,E_{D,\\epsilon}[y\\,\\hat{f}(x;D)]\\\\\n",
    "&=&Var_{\\epsilon}[y]+E_{\\epsilon}[y]^2+Var_{D}[\\hat{f}(x;D)]+E_{D}[\\hat{f}(x;D)]^2-2\\,E_{D,\\epsilon}[f(x)\\,\\hat{f}(x;D)+\\epsilon\\,\\hat{f}(x;D)]\\\\\n",
    "&=&Var_{\\epsilon}[y]+E_{\\epsilon}[y]^2+Var_{D}[\\hat{f}(x;D)]+E_{D}[\\hat{f}(x;D)]^2-2\\,f(x)\\,E_{D}[\\hat{f}(x;D)]-2\\,E_{D,\\epsilon}[\\epsilon\\,\\hat{f}(x;D)]\\\\\n",
    "&=&Var_{\\epsilon}[y]+E_{\\epsilon}[y]^2+Var_{D}[\\hat{f}(x;D)]+E_{D}[\\hat{f}(x;D)]^2-2\\,f(x)\\,E_{D}[\\hat{f}(x;D)]-2\\,E_{\\epsilon}[\\epsilon]E_{D}[\\hat{f}(x;D)]\\\\\n",
    "&=&Var_{\\epsilon}[y]+E_{\\epsilon}[y]^2+Var_{D}[\\hat{f}(x;D)]+E_{D}[\\hat{f}(x;D)]^2-2\\,f(x)\\,E_{D}[\\hat{f}(x;D)]\\\\\n",
    "&=&Var_{\\epsilon}[y]+f(x)^2+Var_{D}[\\hat{f}(x;D)]+E_{D}[\\hat{f}(x;D)]^2-2\\,f(x)\\,E_{D}[\\hat{f}(x;D)]\\\\\n",
    "&=&Var_{\\epsilon}[y]+Var_{D}[\\hat{f}(x;D)]+E_D[\\left(f(x)-\\hat{f}(x;D)\\right)]^2\\\\\n",
    "&=&Var_{\\epsilon}[y]+Var_{D}[\\hat{f}(x;D)]+E_D[\\left(f(x)-\\hat{f}(x;D)\\right)]^2\\\\\n",
    "&=&E_{\\epsilon}[(f(x)+\\epsilon-E_{\\epsilon}[f(x)+\\epsilon])^2]+Var_{D}[\\hat{f}(x;D)]+E_D[\\left(f(x)-\\hat{f}(x;D)\\right)]^2\\\\\n",
    "&=&E_{\\epsilon}[\\epsilon^2]+Var_{D}[\\hat{f}(x;D)]+E_D[\\left(f(x)-\\hat{f}(x;D)\\right)]^2\\\\\n",
    "&=&\\sigma^2+Var_{D}[\\hat{f}(x;D)]+Bias_D[\\hat{f}(x;D)]^2\n",
    "\\end{eqnarray}<br><br>\n",
    "Der erste Term stellt den irreduziblen Fehler, der zweite Term die Varianz und der dritte Term das Quadrat der Verzerrung (Bias) dar. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298cca36-e51d-4ba4-9f3a-b467abd8c22b",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center\"><img src=\"pics/Under_and_Overfitting.png\" alt=\"Bild\" style=\"height: 350px; width: 800px;\"><br>Quelle: sklearn</p><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e95e05-44cf-451a-97c1-174a1144925b",
   "metadata": {},
   "source": [
    "Wie wir nun schon einige male gesehen haben, ist es notwendig die für das überwachte Lernen zur Verfügung stehenden Daten in so genannte Trainings- und Testdaten aufzuteilen. Nur so können wir letztlich die Generalisierungsfähigkeit des Verfahrens bewerten. Wir haben jedoch darüber hinaus auch die Trainingsdaten selbst häufig noch einmal unterteilt in die eigentlichen Trainingsdaten sowie die so genannten Validierungsdaten. Das war notwendig, da die allermeisten Verfahren noch über so genannte Hyperparameter verfügen. Dies sind Parameter, die nicht \"erlernt\" sondern von außen vorgegeben werden müssen (denken Sie bspw. an die Wahl des Splitting-Kriteriums oder die maximale Tiefe bei den Decision Trees). In der Regel liegen keine Heuristiken vor, mit deren Hilfe die optimalen Hyperparameter gefunden werden können. Stattdessen müssen diese experimentell bestimmt werden. Um hierbei ein Overfitting zu vermeiden prüfen wir nach jedem Durchlauf die Performance des Verfahrens auf den so genannten Validierungsdaten. Diese gehen dadurch jedoch auch in den Trainingsvorgang ein und können natürlich nicht mehr für einen finalen Test verwendet werden.<br><br>\n",
    "\n",
    "<p style=\"text-align: center\"><img src=\"pics/grid_search_workflow.png\" alt=\"Bild\" style=\"height: 350px; width: 700px;\"><br>Quelle: sklearn</p><br><br>\n",
    "\n",
    "Ein häufiges Problem, insbesondere beim noch zu besprechenden <i>Deep Learning</i> besteht darin, dass die Anzahl der für das Verfahren notwendigen Trainigsdaten hoch ist und jede Teilung der Datenmenge die für das eigentliche Training zur Verfügung stehende Menge weiter verkleinert. Um diesem Problem zu begegnen besteht die Möglichkeit der <b>Kreuzvalidierung</b> (<b>Cross-Validation</b>).<br><br>\n",
    "\n",
    "<p style=\"text-align: center\"><img src=\"pics/grid_search_cross_validation.png\" alt=\"Bild\" style=\"height: 350px; width: 700px;\"><br>Quelle: sklearn</p><br><br>\n",
    "\n",
    "Nachdem die eigentliche Test-Menge absepariert worden ist, wird die verbliebene Menge in der so genannten <u>$k$-fold cross-validation</u> in $k$ kleinere Mengen aufgeteilt. $k-1$ dieser Menge werden zum Training und genau eine Menge zur Validierung verwendet. Auf diese Weise entstehen $k$ verschiedene Modelle und für jedes Modell erhalten wir unter Verwendung entsprechender Metriken ein Maß der Performance. Um eine Gesamtaussage zu generieren, werden jetzt einfach die Einzelergebnisse gemittelt. Durch die Berechnung der Standardabweichung kann auch die Streuung der Einzelergebnisse mit berücksichtigt werden. Im Extremfall, dem so genannte <b>Leave One Out</b> (<b>LOO</b>), wird jeweils nur ein Datensatz für die Validierung verwendet. Auf diese Weise steigt zwar der Rechenaufwand deutlich an (es müssen mehr Modelle trainiert und analysiert werden), der Umfang der tatsächlichen Trainingsmenge wird so jedoch maximiert.<br> \n",
    "Weitere Variationen dieser Verfahren sowie deren Verwendung in <i>sklearn</i> finden Sie unter <a href=\"https://scikit-learn.org/stable/modules/cross_validation.html\">Cross-validation: evaluating estimator performance</a>. In diesem Zusammenhang soll auch noch auf die Möglichkeit des automatisierten Tunings von Hyperparametern in <i>sklearn</i> hingewiesen werden <a href=\"https://scikit-learn.org/stable/modules/grid_search.html#grid-search\">Tuning the hyper-parameters of an estimator</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f7b457-be6c-4fbf-8536-035b5b092566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
