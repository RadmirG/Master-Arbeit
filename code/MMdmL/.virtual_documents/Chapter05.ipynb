




























































































from IPython.display import IFrame
IFrame(src='https://www.geogebra.org/m/uh8gjse5', width=800, height=600)













































































import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.preprocessing import OneHotEncoder
import sys
sys.path.insert(0, './code/MLP')
from MLP import *

# load dataset
X,y = load_iris(return_X_y=True)

# data splitting (training and test)
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1, random_state=42)

# since y consists only of the label (0,1 or 2) we have to map that to a vector first
y_onehot=OneHotEncoder(sparse=False).fit_transform(y_train.reshape((len(y_train),1)))

# now setup the actual MLP
mlp = MultiLayerPerceptron(np.array([4,10,3]),'meansquared')
mlp.learn(X_train,y_onehot, 0.5, 10000, output_epochs=500)

# Let's check the MLP's performance
y_pred = mlp.predictAll(X_test)

# Since the output values are continues we have to apply a threshold to them first
yy_pred = np.argmax(y_pred,axis=1)
print(yy_pred.shape)

cm = confusion_matrix(y_test, yy_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()














import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import MinMaxScaler
import sys
sys.path.insert(0, './code/MLP')
from MLP import *

# load dataset
X,y = load_iris(return_X_y=True)

# data splitting (training and test)
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1, random_state=42)

# since y consists only of the label (0,1 or 2) we have to map that to a vector first
y_onehot=OneHotEncoder(sparse=False).fit_transform(y_train.reshape((len(y_train),1)))

#scaler = MinMaxScaler()
#scaler.fit(X)
#XX_train=scaler.transform(X_train)
#XX_test=scaler.transform(X_test)

# now setup the actual MLP
mlp = MultiLayerPerceptron(np.array([4,10,3]),'crossentropy')
mlp.learn(X_train,y_onehot, 0.05, 3000, output_epochs=100)

# Let's check the MLP's performance
y_pred = mlp.predictAll(X_test)

# Since the output values are continues we have to apply a threshold to them first
yy_pred = np.argmax(y_pred,axis=1)
print(yy_pred.shape)

cm = confusion_matrix(y_test, yy_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()











# In order to make matplotlib's output dynamical this had to be done: pip install --upgrade jupyterlab ipympl

#%matplotlib widget 
import numpy as np
import sys
sys.path.insert(0, './code/MLP')
from MLP import *
import matplotlib.pyplot as plt
from scipy.stats import *
from ipywidgets import interactive, fixed
import ipywidgets as widgets


# plot of 3D point distribution
cov=np.array([[2,0.8,1.3],[0.8,2,-0.3],[1.3,-0.3,2]])
mean=np.array([0,0,0])
pdf=multivariate_normal(cov=cov, mean=mean,seed=42)
samples=pdf.rvs(size=1000)

mlp = MultiLayerPerceptron(np.array([3,2,3]),'meansquared',activationFunction='linear')
mlp.learn(samples,samples,0.00005,100,output_epochs=101)

# extract vectors spanning the linear manifold
meanVector   = np.zeros(3)
meanVector[0] = mlp.getLayer(2).getPerceptron(0).getBias()
meanVector[1] = mlp.getLayer(2).getPerceptron(1).getBias()
meanVector[2] = mlp.getLayer(2).getPerceptron(2).getBias()

weights1 = mlp.getLayer(2).getPerceptron(0).getWeights()
weights2 = mlp.getLayer(2).getPerceptron(1).getWeights()
weights3 = mlp.getLayer(2).getPerceptron(2).getWeights()

firstVector  = np.array([weights1[0],weights2[0],weights3[0]])
secondVector = np.array([weights1[1],weights2[1],weights3[1]])

soa = np.array([[meanVector[0],meanVector[1], meanVector[2], firstVector[0], firstVector[1], firstVector[2]], [meanVector[0],meanVector[1], meanVector[2], secondVector[0], secondVector[1], secondVector[2]]])
X, Y, Z, U, V, W = zip(*soa)

#%matplotlib widget
fig = plt.figure(1, figsize=(12,12))
ax =plt.axes(projection='3d')
ax.scatter3D(samples[:,0],samples[:,1],samples[:,2])
ax.quiver(X, Y, Z, U, V, W, length=5, color='red')
plt.show()





#%matplotlib widget 
import numpy as np
import sys
sys.path.insert(0, './code/MLP')
from MLP import *
import matplotlib.pyplot as plt
from scipy.stats import *
from sklearn.decomposition import PCA
from ipywidgets import interactive, fixed
import ipywidgets as widgets

def showResults(showPoints=True):
    # plot of 3D point distribution
    cov=np.array([[2,0.8,1.3],[0.8,2,-0.3],[1.3,-0.3,2]])
    mean=np.array([0,0,0])
    pdf=multivariate_normal(cov=cov, mean=mean,seed=42)
    samples=pdf.rvs(size=1000)

    # let's do a pca on these data points
    pca = PCA(n_components=2)
    pca.fit(samples)
    firstAxis  = pca.inverse_transform(np.array([1,0]))
    secondAxis = pca.inverse_transform(np.array([0,1]))

    mlp = MultiLayerPerceptron(np.array([3,2,3]),'meansquared',activationFunction='linear')
    mlp.learn(samples,samples,0.00005,200,output_epochs=201)

    # extract vectors spanning the linear manifold
    meanVector   = np.zeros(3)
    meanVector[0] = mlp.getLayer(2).getPerceptron(0).getBias()
    meanVector[1] = mlp.getLayer(2).getPerceptron(1).getBias()
    meanVector[2] = mlp.getLayer(2).getPerceptron(2).getBias()

    weights1 = mlp.getLayer(2).getPerceptron(0).getWeights()
    weights2 = mlp.getLayer(2).getPerceptron(1).getWeights()
    weights3 = mlp.getLayer(2).getPerceptron(2).getWeights()

    firstVector  = np.array([weights1[0],weights2[0],weights3[0]])
    secondVector = np.array([weights1[1],weights2[1],weights3[1]])

    soa = np.array([[meanVector[0],meanVector[1], meanVector[2], firstVector[0], firstVector[1], firstVector[2]], [meanVector[0],meanVector[1], meanVector[2], secondVector[0], secondVector[1], secondVector[2]]])
    X, Y, Z, U, V, W = zip(*soa)

    soa2 = np.array([[meanVector[0],meanVector[1], meanVector[2], firstAxis[0], firstAxis[1], firstAxis[2]], [meanVector[0],meanVector[1], meanVector[2], secondAxis[0], secondAxis[1], secondAxis[2]]])
    XX, YY, ZZ, UU, VV, WW = zip(*soa2)

    %matplotlib widget
    fig = plt.figure(1, figsize=(12,12))
    ax =plt.axes(projection='3d')
    ax.set_xlim([-5, 5])
    ax.set_ylim([-5, 5])
    ax.set_zlim([-5, 5])
    if (showPoints):
        ax.scatter3D(samples[:,0],samples[:,1],samples[:,2])
    ax.quiver(X, Y, Z, U, V, W, length=5, color='red')
    ax.quiver(XX, YY, ZZ, UU, VV, WW, length=5, color='green')
    plt.show()
    
v=interactive(showResults, showPoints=True)
v      





%matplotlib widget 
import numpy as np
import sys
sys.path.insert(0, './code/MLP')
from MLP import *
import matplotlib.pyplot as plt
from scipy.stats import *
from sklearn.decomposition import PCA
from ipywidgets import interactive, fixed
import ipywidgets as widgets

def showResults(showPoints=True):
    # plot of 3D point distribution
    phis   = uniform.rvs(loc=0, scale=2*np.pi, size=800)
    thetas = uniform.rvs(loc=-np.pi/2, scale=np.pi, size=800)
    X = np.cos(phis)*np.cos(thetas)
    Y = np.sin(phis)*np.cos(thetas)
    Z = np.sin(thetas)
    samples= np.vstack((X,Y,Z)).T
    
    fig = plt.figure(1, figsize=(12,12))
    ax =plt.axes(projection='3d')
    ax.set_xlim([-2, 2])
    ax.set_ylim([-2, 2])
    ax.set_zlim([-2, 2])
    if (showPoints):
        ax.scatter3D(samples[:,0],samples[:,1],samples[:,2])
        #ax.scatter3D(decoded_samples1[:,0], decoded_samples1[:,1], decoded_samples1[:,2], color='red')
        #ax.scatter3D(decoded_samples2[:,0], decoded_samples2[:,1], decoded_samples2[:,2], color='yellow')
    
    
showResults()    


%matplotlib widget 
import numpy as np
import sys
sys.path.insert(0, './code/MLP')
from MLP import *
import matplotlib.pyplot as plt
from scipy.stats import *
from sklearn.decomposition import PCA
from ipywidgets import interactive, fixed
import ipywidgets as widgets

def showResults(showPoints=True):
    # plot of 3D point distribution
    phis   = uniform.rvs(loc=0, scale=2*np.pi, size=800)
    thetas = uniform.rvs(loc=-np.pi/2, scale=np.pi, size=800)
    X = np.cos(phis)*np.cos(thetas)
    Y = np.sin(phis)*np.cos(thetas)
    Z = np.sin(thetas)
    samples= np.vstack((X,Y,Z)).T
    
    fig = plt.figure(1, figsize=(12,12))
    ax =plt.axes(projection='3d')
    ax.set_xlim([-2, 2])
    ax.set_ylim([-2, 2])
    ax.set_zlim([-2, 2])
    mlp = MultiLayerPerceptron(np.array([3,5,2,5,3]),'meansquared',activationFunction='tanh')
    mlp.learn(samples, samples, 0.05, 300, output_epochs=50)
    mlp.learn(samples, samples, 0.005, 300, output_epochs=50)
    mlp.learn(samples, samples, 0.0001, 300, output_epochs=50)
    
    # check to eliminate encoder structure
    encoder_mlp = MultiLayerPerceptron(np.array([3,5,2]),'meansquared',activationFunction='tanh')
    encoder_mlp.layers[0]=mlp.getLayer(0)
    encoder_mlp.layers[1]=mlp.getLayer(1)
    encoder_mlp.layers[2]=mlp.getLayer(2) 

    decoder_mlp = MultiLayerPerceptron(np.array([2,5,3]),'meansquared',activationFunction='tanh')
    decoder_mlp.layers[0]=mlp.getLayer(2)
    decoder_mlp.layers[1]=mlp.getLayer(3)
    decoder_mlp.layers[2]=mlp.getLayer(4)
    
    # create encoded samples
    #encoded_samples = encoder_mlp.predictAll(samples)
    dummy = np.arange(-1,1,0.001)
    zeros = np.zeros(len(dummy))
    encoded_samples1 = np.vstack((dummy,zeros)).T
    encoded_samples2 = np.vstack((zeros, dummy)).T

    decoded_samples1 = decoder_mlp.predictAll(encoded_samples1)
    decoded_samples2 = decoder_mlp.predictAll(encoded_samples2)
    
    if (showPoints):
        ax.scatter3D(samples[:,0],samples[:,1],samples[:,2])
        ax.scatter3D(decoded_samples1[:,0], decoded_samples1[:,1], decoded_samples1[:,2], color='red')
        ax.scatter3D(decoded_samples2[:,0], decoded_samples2[:,1], decoded_samples2[:,2], color='yellow')
    
    
showResults()    










































